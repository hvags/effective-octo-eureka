{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3a47173",
   "metadata": {},
   "source": [
    "# Final Project - Evaluation Run and Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42886b21",
   "metadata": {},
   "source": [
    "## Installing packages (if running on colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28302c7d",
   "metadata": {},
   "source": [
    "**IMPORTANT**: Uncomment and run the cell below and then restart the Runtime (Menu Runtime > Restart Runtime, or with Ctrl + M .), then run it again. If you do not do that, then you will get errors. You only need to run it again if your Google Colab / Kaggle instance is restarted or lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f8bdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "! pip install --upgrade scipy\n",
    "! pip install --upgrade pandas\n",
    "! pip install ipywidgets\n",
    "! pip uninstall -y pykeen\n",
    "! pip install git+https://github.com/pykeen/pykeen.git@v1.5.0\n",
    "! python -c \"import pykeen\" || pip install git+https://github.com/pykeen/pykeen.git@v1.5.0\n",
    "from pkg_resources import require\n",
    "require('pykeen')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b2602f",
   "metadata": {},
   "source": [
    "After you install the packages above, you can just run from this cell onwards (Ctrl + F10 when this is selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb971aea",
   "metadata": {},
   "source": [
    "If SAVE_TO_DRIVE is set to True, the following cell enables storing of hpo results to your google drive account. Authentication required, and only works on colab instances (as far as we know)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "880e5bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_TO_DRIVE = False\n",
    "if (SAVE_TO_DRIVE):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba404ff5",
   "metadata": {},
   "source": [
    "## Imports and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73586bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.pipeline import pipeline_from_config\n",
    "from pykeen.triples import TriplesFactory\n",
    "import os.path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06115113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this value for BASE_DATA_URL if working with local data\n",
    "BASE_DATA_URL = './data'\n",
    "\n",
    "# Use this value for BASE_DATA_URL if working with data from the github repo\n",
    "#BASE_DATA_URL = 'https://raw.githubusercontent.com/hvags/effective-octo-eureka/main/data'\n",
    "\n",
    "\n",
    "# Use this value for HPO_URL_BASE if working with local data\n",
    "HPO_URL_BASE = './hpo_results'\n",
    "\n",
    "# Use this value for HPO_URL_BASE if working with data from the github repo\n",
    "#HPO_URL_BASE = 'https://raw.githubusercontent.com/hvags/effective-octo-eureka/main/hpo_results'\n",
    "\n",
    "HPO_URL_TAIL = 'best_pipeline/pipeline_config.json'\n",
    "\n",
    "# Run on GPU\n",
    "CPU_DEV = 'gpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d5ed13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627fcd4e",
   "metadata": {},
   "source": [
    "## wn18rr datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32bcdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls['wn18rr-full'] = {\n",
    "    'train': '/wn18rr/train_wn18rr.txt',\n",
    "    'validate': '/wn18rr/valid_wn18rr.txt',\n",
    "    'test': '/wn18rr/test_wn18rr.txt'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443164bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls['wn18rr-sym'] = {\n",
    "    'train': '/wn18rr/sym_train_wn18rr.txt',\n",
    "    'validate': '/wn18rr/sym_valid_wn18rr.txt',\n",
    "    'test': '/wn18rr/sym_test_wn18rr.txt'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c96dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls['wn18rr-asym'] = {\n",
    "    'train': '/wn18rr/asym_train_wn18rr.txt',\n",
    "    'validate': '/wn18rr/asym_valid_wn18rr.txt',\n",
    "    'test': '/wn18rr/asym_test_wn18rr.txt'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374db068",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d5a71a",
   "metadata": {},
   "source": [
    "## fb15k-237 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad63b48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls['fb15k-237-full'] = {\n",
    "    'train':  '/fb15k-237/train_fb15k-237.txt',\n",
    "    'validate': '/fb15k-237/valid_fb15k-237.txt',\n",
    "    'test': '/fb15k-237/test_fb15k-237.txt'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc13699",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls['fb15k-237-sym'] = {\n",
    "    'train': '/fb15k-237/sym_train_fb15k-237.txt',\n",
    "    'validate': '/fb15k-237/sym_valid_fb15k-237.txt',\n",
    "    'test': '/fb15k-237/sym_test_fb15k-237.txt'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6528ae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls['fb15k-237-asym'] = {\n",
    "    'train': '/fb15k-237/asym_train_fb15k-237.txt',\n",
    "    'validate': '/fb15k-237/asym_valid_fb15k-237.txt',\n",
    "    'test': '/fb15k-237/asym_test_fb15k-237.txt'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8c8b4f",
   "metadata": {},
   "source": [
    "## Read data from files and create TriplesFactories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f6c09dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: fb15k-237-asym\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're trying to map triples with 11 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 11 from 17011 triples were filtered out\n",
      "You're trying to map triples with 30 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 28 from 19856 triples were filtered out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = dict()\n",
    "\n",
    "for key in dataset_urls.keys():\n",
    "    print(f'Processing: {key}')\n",
    "    \n",
    "    datasets[key] = dict()\n",
    "    \n",
    "    df_train = pd.read_csv(BASE_DATA_URL + dataset_urls[key]['train'], header=None, sep='\\t', names=['head', 'relation','tail'])\n",
    "    df_validate = pd.read_csv(BASE_DATA_URL + dataset_urls[key]['validate'], header=None, sep='\\t', names=['head', 'relation','tail'])\n",
    "    df_test = pd.read_csv(BASE_DATA_URL + dataset_urls[key]['test'], header=None, sep='\\t', names=['head', 'relation','tail'])\n",
    "    \n",
    "    datasets[key]['train'] = TriplesFactory.from_labeled_triples(df_train.astype('str').to_numpy())\n",
    "    entity_mapping = datasets[key]['train'].entity_to_id\n",
    "    relation_mapping = datasets[key]['train'].relation_to_id\n",
    "    \n",
    "    datasets[key]['validate'] = TriplesFactory.from_labeled_triples(df_validate.astype('str').to_numpy(),\n",
    "                                                                    entity_to_id=entity_mapping,\n",
    "                                                                    relation_to_id=relation_mapping\n",
    "                                                                    )\n",
    "    \n",
    "    datasets[key]['test'] = TriplesFactory.from_labeled_triples(df_test.astype('str').to_numpy(),\n",
    "                                                                    entity_to_id=entity_mapping,\n",
    "                                                                    relation_to_id=relation_mapping\n",
    "                                                                    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a299af5",
   "metadata": {},
   "source": [
    "## Models to include in the evaluation\n",
    "\n",
    "If doing a partial run, comment out models that should not be included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c56107",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "          'TransE',\n",
    "          'TransH',\n",
    "          'TransD',\n",
    "          'TransR',\n",
    "          'RESCAL',\n",
    "          'ComplEx',\n",
    "          'RotatE'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28df386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def results_to_table(results):\n",
    "  \n",
    "    # make sure we have a results folder\n",
    "    Path('./results').mkdir(parents=True, exist_ok=True)    \n",
    "    \n",
    "    # select metrics to extract from models\n",
    "    metrics = [('both.realistic.mean_reciprocal_rank', 'MRR'),                  \n",
    "                ('both.realistic.hits@1', 'Hits@1'),                \n",
    "                ('both.realistic.hits@10', 'Hits@10')]                \n",
    "    \n",
    "    for dataset in dataset_urls.keys():\n",
    "        print(dataset)\n",
    "        resultstring = dataset + '\\n'\n",
    "        table = pd.DataFrame()\n",
    "       \n",
    "        for model_label in models:                            \n",
    "            row = [model_label]\n",
    "            \n",
    "            for metric in metrics:      \n",
    "                if ((dataset, model_label) not in results):\n",
    "                    result = 'NaN'\n",
    "                    row += [result]\n",
    "                else:\n",
    "                    result = results[(dataset, model_label)].get_metric(metric[0])\n",
    "                    row += [round(result,5)]                \n",
    "            table = table.append(pd.DataFrame([row], columns =['model'] + [metric[1] for metric in metrics]))\n",
    "\n",
    "        table_string = table.to_string(index=False)\n",
    "        print(table_string)        \n",
    "        print('\\n')\n",
    "        resultstring += table_string + '\\n'\n",
    "        \n",
    "        datestring = datetime.datetime.now().strftime('%Y-%m-%dT%H%M')\n",
    "        with open (f'results/{dataset}_{datestring}.txt', 'a') as file:\n",
    "            file.write(resultstring)\n",
    "        if (SAVE_TO_DRIVE):\n",
    "            Path('/content/drive/MyDrive/results').mkdir(parents=True, exist_ok=True) \n",
    "            with open (f'/content/drive/MyDrive/results/{dataset}_{datestring}.txt', 'a') as file:\n",
    "                file.write(resultstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e7a8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "\n",
    "for dataset_name, dataset_url in dataset_urls.items():    \n",
    "    for model in models:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        url = f'{HPO_URL_BASE}/{dataset_name}/{model}/{HPO_URL_TAIL}'        \n",
    "        \n",
    "        if not (url.startswith('http')):\n",
    "            if not (os.path.exists(url)):\n",
    "                print(url, ' - file not found')\n",
    "                continue\n",
    "            else:\n",
    "                with open(url) as file:\n",
    "                    config = json.load(file)\n",
    "        \n",
    "        else:\n",
    "            response = requests.get(url).text\n",
    "\n",
    "            if (response.startswith('404')):\n",
    "                print(url, response)\n",
    "                continue\n",
    "\n",
    "            config = json.loads(response)\n",
    "\n",
    "        config['pipeline'].pop('testing')\n",
    "        config['pipeline'].pop('training')\n",
    "        config['pipeline'].pop('validation')        \n",
    "        \n",
    "        if (config['pipeline']['loss'] == 'marginranking'):\n",
    "            if('margin_activation' in config['pipeline']['loss_kwargs']):\n",
    "                config['pipeline']['loss_kwargs'].pop('margin_activation')\n",
    "        \n",
    "        dataset = datasets[dataset_name]\n",
    "        print(f'{dataset_name}, {model} loaded')\n",
    "        results[dataset_name, model] = pipeline_from_config(config,\n",
    "                              device=CPU_DEV,\n",
    "                              testing=dataset['test'],\n",
    "                              training=dataset['train'],\n",
    "                              validation=dataset['validate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ba403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_to_table(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
