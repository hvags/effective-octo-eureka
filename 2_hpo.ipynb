{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95564436-b5de-472d-9d76-8a6a05617be3",
   "metadata": {
    "id": "95564436-b5de-472d-9d76-8a6a05617be3"
   },
   "source": [
    "# Final project - Knowledge Graph Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd04c126-6584-418f-a98e-3956f8d8f97b",
   "metadata": {
    "id": "bd04c126-6584-418f-a98e-3956f8d8f97b"
   },
   "source": [
    "## Installing packages (if running on colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebf26d2-6ad5-4b1e-ac81-7b695a276b16",
   "metadata": {
    "id": "1ebf26d2-6ad5-4b1e-ac81-7b695a276b16"
   },
   "source": [
    "**IMPORTANT**: Run the cell below and then restart the Runtime (Menu Runtime > Restart Runtime, or with Ctrl + M .), then run it again. If you do not do that, then you will get errors. You only need to run it again if your Google Colab / Kaggle instance is restarted or lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92d7412a-4532-4bd6-b285-1a2dbe98ee91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92d7412a-4532-4bd6-b285-1a2dbe98ee91",
    "outputId": "45034e4d-b9b9-4a20-be77-3f91534c0910"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[pykeen 1.6.1.dev0 (c:\\users\\hvags\\pykeen\\src),\n",
       " PyYAML 6.0 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " class-resolver 0.1.0 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " docdata 0.0.3 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " pystow 0.2.1 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " more-click 0.0.6 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " tabulate 0.8.9 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " pandas 1.3.4 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " optuna 2.10.0 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " requests 2.26.0 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " tqdm 4.62.3 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " sklearn 0.0 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " click-default-group 1.2.2 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " click 8.0.3 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " scipy 1.7.2 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " numpy 1.21.2 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " dataclasses-json 0.5.6 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " pytz 2021.3 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " numpy 1.21.2 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " python-dateutil 2.8.2 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " cliff 3.9.0 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " scipy 1.7.2 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " packaging 21.2 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " colorlog 6.5.0 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " alembic 1.7.4 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " cmaes 0.8.2 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " SQLAlchemy 1.4.26 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " idna 3.3 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " urllib3 1.26.7 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " charset-normalizer 2.0.7 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " certifi 2021.10.8 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " colorama 0.4.4 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " scikit-learn 1.0.1 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " numpy 1.21.2 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " typing-inspect 0.7.1 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " marshmallow 3.14.0 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " marshmallow-enum 1.5.1 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " six 1.16.0 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " prettytable 2.4.0 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " autopage 0.4.0 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " cmd2 2.2.0 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " pbr 5.7.0 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " PyYAML 6.0 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " stevedore 3.5.0 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " pyparsing 2.4.7 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " pyparsing 2.4.7 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " colorama 0.4.4 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " SQLAlchemy 1.4.26 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " Mako 1.1.5 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " greenlet 1.1.2 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " scipy 1.7.2 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " numpy 1.21.2 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " joblib 1.1.0 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " threadpoolctl 3.0.0 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " typing-extensions 3.10.0.2 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " mypy-extensions 0.4.3 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " marshmallow 3.14.0 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " wcwidth 0.2.5 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " pyperclip 1.8.2 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " wcwidth 0.2.5 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " colorama 0.4.4 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " pyreadline3 3.3 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " attrs 21.2.0 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages),\n",
       " MarkupSafe 2.0.1 (c:\\appl\\anaconda3\\envs\\cuda\\lib\\site-packages)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip install --upgrade scipy\n",
    "! pip install --upgrade pandas\n",
    "! pip install ipywidgets\n",
    "! pip uninstall -y pykeen\n",
    "! pip install git+https://github.com/pykeen/pykeen.git@v1.5.0\n",
    "! python -c \"import pykeen\" || pip install git+https://github.com/pykeen/pykeen.git@v1.5.0\n",
    "from pkg_resources import require\n",
    "require('pykeen')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zlvyH72mnCQZ",
   "metadata": {
    "id": "zlvyH72mnCQZ"
   },
   "source": [
    "After you install the packages above, you can just run from this cell onwards (Ctrl + F10 when this is selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aa8990-f477-4544-8ea3-28eca654036c",
   "metadata": {
    "id": "22aa8990-f477-4544-8ea3-28eca654036c"
   },
   "source": [
    "## Imports and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d0e08e0-21fa-4c4e-94e7-65b113433dd1",
   "metadata": {
    "id": "2d0e08e0-21fa-4c4e-94e7-65b113433dd1"
   },
   "outputs": [],
   "source": [
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.triples import TriplesFactory\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6859571b-025a-45e7-9c46-e9794f45d03f",
   "metadata": {
    "id": "6859571b-025a-45e7-9c46-e9794f45d03f"
   },
   "source": [
    "How many epochs to use on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78042b10-fb4b-4b38-90a7-09b893995dc0",
   "metadata": {
    "id": "78042b10-fb4b-4b38-90a7-09b893995dc0"
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r-GxRe-eSiYB",
   "metadata": {
    "id": "r-GxRe-eSiYB"
   },
   "source": [
    "How many seconds to use for each hyperparameter optimization trial. The optimizer will run as many trials as possible and stop after finishing the one that was runningwhen time ran out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "niWdghjsSnkR",
   "metadata": {
    "id": "niWdghjsSnkR"
   },
   "outputs": [],
   "source": [
    "TIME_PER_TRIAL = 900"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cedb8c-a8d8-45fa-8a5f-a4a9103f6d54",
   "metadata": {
    "id": "27cedb8c-a8d8-45fa-8a5f-a4a9103f6d54"
   },
   "source": [
    "Always run on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0838dfb-8977-4557-a47a-5fff52a72e63",
   "metadata": {
    "id": "e0838dfb-8977-4557-a47a-5fff52a72e63"
   },
   "outputs": [],
   "source": [
    "CPU_DEV = 'gpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fWxLIrFat5jm",
   "metadata": {
    "id": "fWxLIrFat5jm"
   },
   "source": [
    "Regularizer weight bounds for hyper parametr optimalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "M1oOC7VOt-sF",
   "metadata": {
    "id": "M1oOC7VOt-sF"
   },
   "outputs": [],
   "source": [
    "REGULARIZER_LOW = 0.0001\n",
    "REGULARIZER_HIGH = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67105167-2968-4965-bb4f-da544251a0e8",
   "metadata": {
    "id": "67105167-2968-4965-bb4f-da544251a0e8"
   },
   "source": [
    "## Retrieving data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb71192-0de3-4814-90dd-dbc927e3bb52",
   "metadata": {
    "id": "feb71192-0de3-4814-90dd-dbc927e3bb52"
   },
   "source": [
    "The data sets used were already pre stratified into training, testing and validation sets. Using the methods described in the data manipulation notebook they were split into symmetrical and asymmetrical subsets. The results were stored in a github repository for easy access in the further work.\n",
    "\n",
    "To retrieve the data we first populate a dictionary with the URL of the sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caac382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "532af1a6",
   "metadata": {
    "id": "532af1a6"
   },
   "outputs": [],
   "source": [
    "# wn18rr datasets\n",
    "dataset_urls['wn18rr-full'] = {\n",
    "    'train': 'https://raw.githubusercontent.com/hvags/effective-octo-eureka/main/wn18rr/train_wn18rr.txt',\n",
    "    'validate': 'https://raw.githubusercontent.com/hvags/effective-octo-eureka/main/wn18rr/valid_wn18rr.txt',\n",
    "    'test': 'https://raw.githubusercontent.com/hvags/effective-octo-eureka/main/wn18rr/test_wn18rr.txt'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da71dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls['wn18rr-sym'] = {\n",
    "    'train': 'https://raw.githubusercontent.com/hvags/effective-octo-eureka/main/wn18rr/sym_train_wn18rr.txt',\n",
    "    'validate': 'https://raw.githubusercontent.com/hvags/effective-octo-eureka/main/wn18rr/sym_valid_wn18rr.txt',\n",
    "    'test': 'https://raw.githubusercontent.com/hvags/effective-octo-eureka/main/wn18rr/sym_test_wn18rr.txt'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73128fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls['wn18rr-asym'] = {\n",
    "    'train': 'https://raw.githubusercontent.com/hvags/effective-octo-eureka/main/wn18rr/asym_train_wn18rr.txt',\n",
    "    'validate': 'https://raw.githubusercontent.com/hvags/effective-octo-eureka/main/wn18rr/asym_valid_wn18rr.txt',\n",
    "    'test':'https://raw.githubusercontent.com/hvags/effective-octo-eureka/main/wn18rr/asym_test_wn18rr.txt'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x6HEzrD9Y3Py",
   "metadata": {
    "id": "x6HEzrD9Y3Py"
   },
   "outputs": [],
   "source": [
    "# fb15k-237 datasets\n",
    "dataset_urls['fb15k-237-full'] = {\n",
    "    'train': 'https://raw.githubusercontent.com/hvags/effective-octo-eureka/main/fb15k-237/train.txt',\n",
    "    'validate': 'https://raw.githubusercontent.com/hvags/effective-octo-eureka/main/fb15k-237/valid.txt',\n",
    "    'test': 'https://raw.githubusercontent.com/hvags/effective-octo-eureka/main/fb15k-237/test.txt'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df37e284",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls['fb15k-237-sym'] = {\n",
    "    'train': 'https://raw.githubusercontent.com/hvags/effective-octo-eureka/main/fb15k-237/sym_train.txt',\n",
    "    'validate': 'https://raw.githubusercontent.com/hvags/effective-octo-eureka/main/fb15k-237/sym_valid.txt',\n",
    "    'test': 'https://raw.githubusercontent.com/hvags/effective-octo-eureka/main/fb15k-237/sym_test.txt'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ede1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls['fb15k-237-asym'] = {\n",
    "    'train': 'https://raw.githubusercontent.com/hvags/effective-octo-eureka/main/fb15k-237/asym_train.txt',\n",
    "    'validate': 'https://raw.githubusercontent.com/hvags/effective-octo-eureka/main/fb15k-237/asym_valid.txt',\n",
    "    'test': 'https://raw.githubusercontent.com/hvags/effective-octo-eureka/main/fb15k-237/asym_test.txt'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c356e4f5",
   "metadata": {
    "id": "c356e4f5"
   },
   "source": [
    "We then read the data from the above URLs into a dataset dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fe2022a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7fe2022a",
    "outputId": "0ca75080-286e-4018-f247-353595bcf710"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: wn18rr-full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're trying to map triples with 211 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 210 from 3034 triples were filtered out\n",
      "You're trying to map triples with 212 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 210 from 3134 triples were filtered out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = dict()\n",
    "\n",
    "for key in dataset_urls.keys():\n",
    "    print(f'Processing: {key}')\n",
    "    \n",
    "    datasets[key] = dict()\n",
    "    \n",
    "    df_train = pd.read_csv(dataset_urls[key]['train'], header=None, sep='\\t', names=['head', 'relation','tail'])\n",
    "    df_validate = pd.read_csv(dataset_urls[key]['validate'], header=None, sep='\\t', names=['head', 'relation','tail'])\n",
    "    df_test = pd.read_csv(dataset_urls[key]['test'], header=None, sep='\\t', names=['head', 'relation','tail'])\n",
    "    \n",
    "    datasets[key]['train'] = TriplesFactory.from_labeled_triples(df_train.astype('str').to_numpy())\n",
    "    entity_mapping = datasets[key]['train'].entity_to_id\n",
    "    relation_mapping = datasets[key]['train'].relation_to_id\n",
    "    \n",
    "    datasets[key]['validate'] = TriplesFactory.from_labeled_triples(df_validate.astype('str').to_numpy(),\n",
    "                                                                    entity_to_id=entity_mapping,\n",
    "                                                                    relation_to_id=relation_mapping\n",
    "                                                                    )\n",
    "    \n",
    "    datasets[key]['test'] = TriplesFactory.from_labeled_triples(df_test.astype('str').to_numpy(),\n",
    "                                                                    entity_to_id=entity_mapping,\n",
    "                                                                    relation_to_id=relation_mapping\n",
    "                                                                    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f04a673-f79c-42fd-9df9-2c2a34c2c5ea",
   "metadata": {
    "id": "6f04a673-f79c-42fd-9df9-2c2a34c2c5ea"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933ca0c9-396d-4849-8054-f3bcf5ee2151",
   "metadata": {
    "id": "933ca0c9-396d-4849-8054-f3bcf5ee2151"
   },
   "source": [
    "#### Defining models and their parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "204d2f03-177f-4c36-ba0f-28c1ea7d007a",
   "metadata": {
    "id": "204d2f03-177f-4c36-ba0f-28c1ea7d007a"
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "          ('TransE', dict(scoring_fct_norm=2)),\n",
    "#          ('TransH', dict()),\n",
    "#          ('TransD', dict()),\n",
    "#          ('TransR', dict()),\n",
    "#          ('RESCAL', dict()),\n",
    "#          ('ComplEx', dict()),\n",
    "#          ('RotatE', dict())\n",
    "         ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rL9cGetPMPCf",
   "metadata": {
    "id": "rL9cGetPMPCf"
   },
   "source": [
    "Define parameters that are specific to a combination of dataset and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "FlZRIRaNMOgs",
   "metadata": {
    "id": "FlZRIRaNMOgs"
   },
   "outputs": [],
   "source": [
    "class md_param:\n",
    "    def __init__(self, loss, training_loop):\n",
    "        self.loss = loss\n",
    "        self.training_loop = training_loop        \n",
    "\n",
    "\n",
    "model_dataset_param = {('TransE',  'wn18rr'):     md_param('BCEWithLogitsLoss', 'lcwa'),\n",
    "                       ('TransH',  'wn18rr'):     md_param('MarginRankingLoss', 'slcwa'),\n",
    "                       ('TransD',  'wn18rr'):     md_param('MarginRankingLoss', 'slcwa'),\n",
    "                       ('TransR',  'wn18rr'):     md_param('MarginRankingLoss', 'slcwa'),\n",
    "                       ('RESCAL',  'wn18rr'):     md_param('CrossEntropyLoss', 'lcwa'),\n",
    "                       ('ComplEx', 'wn18rr'):     md_param('CrossEntropyLoss', 'lcwa'),\n",
    "                       ('RotatE',  'wn18rr'):     md_param('BCEWithLogitsLoss', 'lcwa'),\n",
    "\n",
    "                       ('TransE',  'fb15k-237'):  md_param('MarginRankingLoss', 'slcwa'),\n",
    "                       ('TransH',  'fb15k-237'):  md_param('MarginRankingLoss', 'slcwa'),\n",
    "                       ('TransD',  'fb15k-237'):  md_param('MarginRankingLoss', 'slcwa'),\n",
    "                       ('TransR',  'fb15k-237'):  md_param('CrossEntropyLoss', 'lcwa'),\n",
    "                       ('RESCAL',  'fb15k-237'):  md_param('CrossEntropyLoss', 'lcwa'),                       \n",
    "                       ('ComplEx', 'fb15k-237'):  md_param('CrossEntropyLoss', 'lcwa'),\n",
    "                       ('RotatE',  'fb15k-237'):  md_param('NSSALoss', 'lcwa'),\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5TmfrEU8Dqf7",
   "metadata": {
    "id": "5TmfrEU8Dqf7"
   },
   "outputs": [],
   "source": [
    "from pykeen.hpo import hpo_pipeline\n",
    "    \n",
    "def run_hpo_pipeline(dataset_name, dataset, model):\n",
    "  model_name = model[0]\n",
    "  model_params = model[1]\n",
    "  dataset_base = str.join('-',str.split(dataset_name, '-')[:-1])\n",
    "  print(f\"Dataset: {dataset_name}, Model: {model_name}\")\n",
    "  result = hpo_pipeline(  \n",
    "    timeout=TIME_PER_TRIAL,\n",
    "    training=dataset['train'],\n",
    "    testing=dataset['test'],\n",
    "    validation=dataset['validate'],\n",
    "    model=model_name,\n",
    "    model_kwargs=model_params,\n",
    "    # set the parameters specific to this combination of dataset and model\n",
    "    loss=model_dataset_param[model_name, dataset_base].loss,\n",
    "    training_loop=model_dataset_param[model_name, dataset_base].training_loop,\n",
    "    training_kwargs=dict(num_epochs=N_EPOCHS,\n",
    "                         use_tqdm_batch=False),\n",
    "    training_kwargs_ranges=dict(\n",
    "                            batch_size=dict(type=int, low=256, high=1024, q=256),                    \n",
    "                            ),\n",
    "    device=CPU_DEV,\n",
    "    metric='MEAN_RECIPROCAL_RANK',\n",
    "    direction='maximize',\n",
    "    stopper='early',\n",
    "    stopper_kwargs=dict(frequency=10, patience=2, relative_delta=0.01),\n",
    "    model_kwargs_ranges=dict(\n",
    "                        embedding_dim=dict(type=int, low=32, high=400, q=64)\n",
    "                        )\n",
    "    ) \n",
    "\n",
    "  return(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "debdc3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57f19f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b382dcd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4d67abbcba2644dd848a4dc59d440e10",
      "8248daea375847e4ae88814476a1b9a8",
      "9e465cc59c484e3f9c27e49eacd467ac",
      "567c948e45954066a4aa7a1a89098a42",
      "de412ff7cc664d7980f1b3e6599ccb29",
      "a266d7e9dab7475d9963e58b59618cf3",
      "d3650082397d47f5990c719da58ac988",
      "e439e21205ad402fb0b1cf03117049f8",
      "e7b0b0ee7807451f9bc34772b87d6dfe",
      "5c3e0eeabb28403b9d785f069fbff219",
      "49a44d4eeabe40ffb2f45d393a1ab44f",
      "953896fd4f474038943e2eb793cb381e",
      "a1d72432d1d74130a8245a4cbd7b3a34",
      "c5682eabe1ac4c0a98e57004e7d5e409",
      "67c18d106aaf4ae7baa9b230ae3fdd02",
      "027c176eb68c4f2dac2b2f3217e318ce",
      "830b475f509843afbdd708da9ea0cfe7",
      "1a1d8025f0a744988f99e393f5eb0e9d",
      "69379bacf88a442cb9c84a5a96879570",
      "2ed88daadce14e20ba6c448f7d938e76",
      "d9c7bc2a441b420e9a960d001f465b16",
      "bb564f5ffe4e40baac80b021a03c6c88",
      "bcf151eab4214bdf84dfcdfb4af91cb9",
      "bdf16783cbce4e548068fd46349baef5",
      "de343a9d40ea48baa0dd128601575eaa",
      "90a573216fd44732b9eb9cacf9268236",
      "b4b7a8209c2045b3acbd56f921d3d7c9",
      "186964fcc07e4d9fbbcad9095cff6ae6",
      "80668726066d4720b512a185de93ba41"
     ]
    },
    "id": "5b382dcd",
    "outputId": "510305c3-716c-43e4-9e86-4e95e86f815d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-11-09 17:20:38,592]\u001b[0m A new study created in memory with name: no-name-d71c1169-1d11-4a98-9fd7-6493aaebb4fb\u001b[0m\n",
      "INFO:pykeen.hpo.hpo:Using model: <class 'pykeen.models.unimodal.trans_e.TransE'>\n",
      "INFO:pykeen.hpo.hpo:Using loss: <class 'pykeen.losses.BCEWithLogitsLoss'>\n",
      "INFO:pykeen.hpo.hpo:Using optimizer: <class 'torch.optim.adam.Adam'>\n",
      "INFO:pykeen.hpo.hpo:Using training loop: <class 'pykeen.training.lcwa.LCWATrainingLoop'>\n",
      "INFO:pykeen.hpo.hpo:Using evaluator: <class 'pykeen.evaluation.rank_based_evaluator.RankBasedEvaluator'>\n",
      "INFO:pykeen.hpo.hpo:Attempting to maximize MEAN_RECIPROCAL_RANK\n",
      "INFO:pykeen.hpo.hpo:Filter validation triples when testing: True\n",
      "C:\\Appl\\Anaconda3\\envs\\cuda\\lib\\site-packages\\optuna\\distributions.py:560: UserWarning: The distribution is specified by [32, 400] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 352].\n",
      "  warnings.warn(\n",
      "WARNING:pykeen.pipeline.api:No random seed is specified. Setting to 3866378461.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wn18rr-full\n",
      "Dataset: wn18rr-full, Model: TransE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.training.training_loop:Starting sub_batch_size search for training now...\n",
      "INFO:pykeen.training.training_loop:Concluded search with sub_batch_size 12.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e128b4a20dc4159be4ca0c4d9b337e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs on cuda:   0%|          | 0/100 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24188/557617801.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_hpo_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_to_directory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{dataset_name}/{model[0]}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24188/1405797718.py\u001b[0m in \u001b[0;36mrun_hpo_pipeline\u001b[1;34m(dataset_name, dataset, model)\u001b[0m\n\u001b[0;32m      6\u001b[0m   \u001b[0mdataset_base\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Dataset: {dataset_name}, Model: {model_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m   result = hpo_pipeline(  \n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTIME_PER_TRIAL\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hvags\\pykeen\\src\\pykeen\\hpo\\hpo.py\u001b[0m in \u001b[0;36mhpo_pipeline\u001b[1;34m(dataset, dataset_kwargs, training, testing, validation, evaluation_entity_whitelist, evaluation_relation_whitelist, model, model_kwargs, model_kwargs_ranges, loss, loss_kwargs, loss_kwargs_ranges, regularizer, regularizer_kwargs, regularizer_kwargs_ranges, optimizer, optimizer_kwargs, optimizer_kwargs_ranges, lr_scheduler, lr_scheduler_kwargs, lr_scheduler_kwargs_ranges, training_loop, training_loop_kwargs, negative_sampler, negative_sampler_kwargs, negative_sampler_kwargs_ranges, epochs, training_kwargs, training_kwargs_ranges, stopper, stopper_kwargs, evaluator, evaluator_kwargs, evaluation_kwargs, metric, filter_validation_when_testing, result_tracker, result_tracker_kwargs, device, storage, sampler, sampler_kwargs, pruner, pruner_kwargs, study_name, direction, load_if_exists, n_trials, timeout, n_jobs, save_model_directory)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    801\u001b[0m     \u001b[1;31m# Invoke optimization of the objective function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 802\u001b[1;33m     study.optimize(\n\u001b[0m\u001b[0;32m    803\u001b[0m         \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTrial\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobjective\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m         \u001b[0mn_trials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Appl\\Anaconda3\\envs\\cuda\\lib\\site-packages\\optuna\\study\\study.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    398\u001b[0m             )\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m         _optimize(\n\u001b[0m\u001b[0;32m    401\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Appl\\Anaconda3\\envs\\cuda\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             _optimize_sequential(\n\u001b[0m\u001b[0;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Appl\\Anaconda3\\envs\\cuda\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mtrial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Appl\\Anaconda3\\envs\\cuda\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0mvalue_or_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hvags\\pykeen\\src\\pykeen\\hpo\\hpo.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m             result = pipeline(\n\u001b[0m\u001b[0;32m    242\u001b[0m                 \u001b[1;31m# 1. Dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hvags\\pykeen\\src\\pykeen\\pipeline\\api.py\u001b[0m in \u001b[0;36mpipeline\u001b[1;34m(dataset, dataset_kwargs, training, testing, validation, evaluation_entity_whitelist, evaluation_relation_whitelist, model, model_kwargs, interaction, interaction_kwargs, dimensions, loss, loss_kwargs, regularizer, regularizer_kwargs, optimizer, optimizer_kwargs, clear_optimizer, lr_scheduler, lr_scheduler_kwargs, training_loop, training_loop_kwargs, negative_sampler, negative_sampler_kwargs, epochs, training_kwargs, stopper, stopper_kwargs, evaluator, evaluator_kwargs, evaluation_kwargs, result_tracker, result_tracker_kwargs, metadata, device, random_seed, use_testing_data, evaluation_fallback, filter_validation_when_testing, use_tqdm)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[1;31m# Train like Cristiano Ronaldo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1075\u001b[0m     \u001b[0mtraining_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1076\u001b[1;33m     losses = training_loop_instance.train(\n\u001b[0m\u001b[0;32m   1077\u001b[0m         \u001b[0mtriples_factory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m         \u001b[0mstopper\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstopper_instance\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hvags\\pykeen\\src\\pykeen\\training\\training_loop.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, triples_factory, num_epochs, batch_size, slice_size, label_smoothing, sampler, continue_training, only_size_probing, use_tqdm, use_tqdm_batch, tqdm_kwargs, stopper, result_tracker, sub_batch_size, num_workers, clear_optimizer, checkpoint_directory, checkpoint_name, checkpoint_frequency, checkpoint_on_failure, drop_last, callbacks, gradient_clipping_max_norm, gradient_clipping_norm_type, gradient_clipping_max_abs_value)\u001b[0m\n\u001b[0;32m    340\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses_per_epochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m             result = self._train(\n\u001b[0m\u001b[0;32m    343\u001b[0m                 \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hvags\\pykeen\\src\\pykeen\\training\\training_loop.py\u001b[0m in \u001b[0;36m_train\u001b[1;34m(self, triples_factory, training_instances, num_epochs, batch_size, slice_size, label_smoothing, sampler, continue_training, only_size_probing, use_tqdm, use_tqdm_batch, tqdm_kwargs, stopper, result_tracker, sub_batch_size, num_workers, save_checkpoints, checkpoint_path, checkpoint_frequency, checkpoint_on_failure_file_path, best_epoch_model_file_path, last_best_epoch, drop_last, callbacks, gradient_clipping_max_norm, gradient_clipping_norm_type, gradient_clipping_max_abs_value)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m                         \u001b[1;31m# forward pass call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 619\u001b[1;33m                         batch_loss = self._forward_pass(\n\u001b[0m\u001b[0;32m    620\u001b[0m                             \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m                             \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hvags\\pykeen\\src\\pykeen\\training\\training_loop.py\u001b[0m in \u001b[0;36m_forward_pass\u001b[1;34m(self, batch, start, stop, current_batch_size, label_smoothing, slice_size)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m         \u001b[1;31m# raise error when non-finite loss occurs (NaN, +/-inf)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 782\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    783\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mNonFiniteLossError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loss is non-finite.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset in datasets.items():\n",
    "  print(dataset_name)\n",
    "\n",
    "  for model in models:\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    result = run_hpo_pipeline(dataset_name, dataset, model)\n",
    "    result.save_to_directory(f'{dataset_name}/{model[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "McGjcHbhS6JB",
   "metadata": {
    "id": "McGjcHbhS6JB"
   },
   "outputs": [],
   "source": [
    "result.save_to_directory(f'{dataset_name}/{model[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DKfR_rEn2x1G",
   "metadata": {
    "id": "DKfR_rEn2x1G"
   },
   "outputs": [],
   "source": [
    "def result_to_table(studies):\n",
    "\n",
    "  metrics = [('both.realistic.arithmetic_mean_rank', 'MR'),  \n",
    "            ('both.realistic.adjusted_arithmetic_mean_rank_index', 'AMRI'),\n",
    "            ('both.realistic.inverse_geometric_mean_rank', 'IGMR'),\n",
    "            ('both.realistic.hits_at_1', 'Hits@1'),\n",
    "            ('both.realistic.hits_at_3', 'Hits@3'),\n",
    "            ('both.realistic.hits_at_5', 'Hits@5')]\n",
    "\n",
    "  model_labels = [x[0] for x in models]           \n",
    "\n",
    "  for dataset in datasets.keys():\n",
    "    table = pd.DataFrame(columns = ['metric'] + model_labels)\n",
    "        \n",
    "    for metric in metrics:\n",
    "      row = [metric[1]]\n",
    "      for model_label in model_labels:        \n",
    "        result = studies[(dataset, model_label)].best_trial.user_attrs[metric[0]]\n",
    "        row += [round(result,5)]\n",
    "      table = table.append(pd.DataFrame([row], columns =['metric'] + model_labels ))\n",
    "\n",
    "    print(f'\\n\\nScores for the {dataset} dataset.\\n')\n",
    "    print(table.to_string(index=False))\n",
    "\n",
    "def print_best_params(model_name, study):        \n",
    "    print(f'\\nbest hyperparameters found for {model_name}')                        \n",
    "    print(pd.DataFrame.from_dict(study.best_params, orient='index').to_string())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Final Project - Embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
