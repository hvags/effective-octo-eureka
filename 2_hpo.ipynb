{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eebdbf49",
   "metadata": {
    "id": "95564436-b5de-472d-9d76-8a6a05617be3"
   },
   "source": [
    "# Final project - Hyper parameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd04c126-6584-418f-a98e-3956f8d8f97b",
   "metadata": {
    "id": "bd04c126-6584-418f-a98e-3956f8d8f97b"
   },
   "source": [
    "## Installing packages (if running on colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebf26d2-6ad5-4b1e-ac81-7b695a276b16",
   "metadata": {
    "id": "1ebf26d2-6ad5-4b1e-ac81-7b695a276b16"
   },
   "source": [
    "**IMPORTANT**: Run the cell below and then restart the Runtime (Menu Runtime > Restart Runtime, or with Ctrl + M .), then run it again. If you do not do that, then you will get errors. You only need to run it again if your Google Colab / Kaggle instance is restarted or lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d7412a-4532-4bd6-b285-1a2dbe98ee91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92d7412a-4532-4bd6-b285-1a2dbe98ee91",
    "outputId": "45034e4d-b9b9-4a20-be77-3f91534c0910"
   },
   "outputs": [],
   "source": [
    "! pip install --upgrade scipy\n",
    "! pip install --upgrade pandas\n",
    "! pip install ipywidgets\n",
    "! pip uninstall -y pykeen\n",
    "! pip install git+https://github.com/pykeen/pykeen.git@v1.5.0\n",
    "! python -c \"import pykeen\" || pip install git+https://github.com/pykeen/pykeen.git@v1.5.0\n",
    "from pkg_resources import require\n",
    "require('pykeen')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zlvyH72mnCQZ",
   "metadata": {
    "id": "zlvyH72mnCQZ"
   },
   "source": [
    "After you install the packages above, you can just run from this cell onwards (Ctrl + F10 when this is selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aa8990-f477-4544-8ea3-28eca654036c",
   "metadata": {
    "id": "22aa8990-f477-4544-8ea3-28eca654036c"
   },
   "source": [
    "## Imports and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d0e08e0-21fa-4c4e-94e7-65b113433dd1",
   "metadata": {
    "id": "2d0e08e0-21fa-4c4e-94e7-65b113433dd1"
   },
   "outputs": [],
   "source": [
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.triples import TriplesFactory\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6859571b-025a-45e7-9c46-e9794f45d03f",
   "metadata": {
    "id": "6859571b-025a-45e7-9c46-e9794f45d03f"
   },
   "source": [
    "How many epochs to use on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78042b10-fb4b-4b38-90a7-09b893995dc0",
   "metadata": {
    "id": "78042b10-fb4b-4b38-90a7-09b893995dc0"
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r-GxRe-eSiYB",
   "metadata": {
    "id": "r-GxRe-eSiYB"
   },
   "source": [
    "How many seconds to use for each hyperparameter optimization trial. The optimizer will run as many trials as possible and stop after finishing the one that was runningwhen time ran out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "niWdghjsSnkR",
   "metadata": {
    "id": "niWdghjsSnkR"
   },
   "outputs": [],
   "source": [
    "TIME_PER_TRIAL = 900"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cedb8c-a8d8-45fa-8a5f-a4a9103f6d54",
   "metadata": {
    "id": "27cedb8c-a8d8-45fa-8a5f-a4a9103f6d54"
   },
   "source": [
    "Always run on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0838dfb-8977-4557-a47a-5fff52a72e63",
   "metadata": {
    "id": "e0838dfb-8977-4557-a47a-5fff52a72e63"
   },
   "outputs": [],
   "source": [
    "CPU_DEV = 'gpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fWxLIrFat5jm",
   "metadata": {
    "id": "fWxLIrFat5jm"
   },
   "source": [
    "Regularizer weight bounds for hyper parametr optimalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "M1oOC7VOt-sF",
   "metadata": {
    "id": "M1oOC7VOt-sF"
   },
   "outputs": [],
   "source": [
    "REGULARIZER_LOW = 0.0001\n",
    "REGULARIZER_HIGH = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67105167-2968-4965-bb4f-da544251a0e8",
   "metadata": {
    "id": "67105167-2968-4965-bb4f-da544251a0e8"
   },
   "source": [
    "## Retrieving data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb71192-0de3-4814-90dd-dbc927e3bb52",
   "metadata": {
    "id": "feb71192-0de3-4814-90dd-dbc927e3bb52"
   },
   "source": [
    "The data sets used were already pre stratified into training, testing and validation sets. Using the methods described in the data manipulation notebook they were split into symmetrical and asymmetrical subsets. The results were stored in a github repository for easy access in the further work.\n",
    "\n",
    "To retrieve the data we first populate a dictionary with the URL of the sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caac382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls = dict()\n",
    "\n",
    "# Use this value for BASE_DATA_URL if working with local data\n",
    "#BASE_DATA_URL = './data'\n",
    "\n",
    "# Use this value for BASE_DATA_URL if working with data from the github repo\n",
    "BASE_DATA_URL = 'https://raw.githubusercontent.com/hvags/effective-octo-eureka/main/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "532af1a6",
   "metadata": {
    "id": "532af1a6"
   },
   "outputs": [],
   "source": [
    "# wn18rr datasets\n",
    "dataset_urls['wn18rr-full'] = {\n",
    "    'train': '/wn18rr/train_wn18rr.txt',\n",
    "    'validate': '/wn18rr/valid_wn18rr.txt',\n",
    "    'test': '/wn18rr/test_wn18rr.txt'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9da71dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls['wn18rr-sym'] = {\n",
    "    'train': '/wn18rr/sym_train_wn18rr.txt',\n",
    "    'validate': '/wn18rr/sym_valid_wn18rr.txt',\n",
    "    'test': '/wn18rr/sym_test_wn18rr.txt'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73128fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls['wn18rr-asym'] = {\n",
    "    'train': '/wn18rr/asym_train_wn18rr.txt',\n",
    "    'validate': '/wn18rr/asym_valid_wn18rr.txt',\n",
    "    'test': '/wn18rr/asym_test_wn18rr.txt'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "x6HEzrD9Y3Py",
   "metadata": {
    "id": "x6HEzrD9Y3Py"
   },
   "outputs": [],
   "source": [
    "# fb15k-237 datasets\n",
    "dataset_urls['fb15k-237-full'] = {\n",
    "    'train':  '/fb15k-237/train_fb15k-237.txt',\n",
    "    'validate': '/fb15k-237/valid_fb15k-237.txt',\n",
    "    'test': '/fb15k-237/test_fb15k-237.txt'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df37e284",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls['fb15k-237-sym'] = {\n",
    "    'train': '/fb15k-237/sym_train_fb15k-237.txt',\n",
    "    'validate': '/fb15k-237/sym_valid_fb15k-237.txt',\n",
    "    'test': '/fb15k-237/sym_test_fb15k-237.txt'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22ede1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls['fb15k-237-asym'] = {\n",
    "    'train': '/fb15k-237/asym_train_fb15k-237.txt',\n",
    "    'validate': '/fb15k-237/asym_valid_fb15k-237.txt',\n",
    "    'test': '/fb15k-237/asym_test_fb15k-237.txt'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c356e4f5",
   "metadata": {
    "id": "c356e4f5"
   },
   "source": [
    "We then read the data from the above URLs into a dataset dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fe2022a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7fe2022a",
    "outputId": "0ca75080-286e-4018-f247-353595bcf710"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: wn18rr-full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're trying to map triples with 211 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 210 from 3034 triples were filtered out\n",
      "You're trying to map triples with 212 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 210 from 3134 triples were filtered out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Processing: wn18rr-sym\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're trying to map triples with 60 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 48 from 1165 triples were filtered out\n",
      "You're trying to map triples with 35 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 31 from 1172 triples were filtered out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Processing: wn18rr-asym\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're trying to map triples with 527 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 518 from 1869 triples were filtered out\n",
      "You're trying to map triples with 555 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 546 from 1962 triples were filtered out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Processing: fb15k-237-full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're trying to map triples with 9 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 9 from 17535 triples were filtered out\n",
      "You're trying to map triples with 30 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 28 from 20466 triples were filtered out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Processing: fb15k-237-sym\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're trying to map triples with 192 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 110 from 524 triples were filtered out\n",
      "You're trying to map triples with 174 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 98 from 610 triples were filtered out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Processing: fb15k-237-asym\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're trying to map triples with 11 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 11 from 17011 triples were filtered out\n",
      "You're trying to map triples with 30 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 28 from 19856 triples were filtered out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = dict()\n",
    "\n",
    "for key in dataset_urls.keys():\n",
    "    print(f'Processing: {key}')\n",
    "    \n",
    "    datasets[key] = dict()\n",
    "    \n",
    "    df_train = pd.read_csv(BASE_DATA_URL + dataset_urls[key]['train'], header=None, sep='\\t', names=['head', 'relation','tail'])\n",
    "    df_validate = pd.read_csv(BASE_DATA_URL + dataset_urls[key]['validate'], header=None, sep='\\t', names=['head', 'relation','tail'])\n",
    "    df_test = pd.read_csv(BASE_DATA_URL + dataset_urls[key]['test'], header=None, sep='\\t', names=['head', 'relation','tail'])\n",
    "    \n",
    "    datasets[key]['train'] = TriplesFactory.from_labeled_triples(df_train.astype('str').to_numpy())\n",
    "    entity_mapping = datasets[key]['train'].entity_to_id\n",
    "    relation_mapping = datasets[key]['train'].relation_to_id\n",
    "    \n",
    "    datasets[key]['validate'] = TriplesFactory.from_labeled_triples(df_validate.astype('str').to_numpy(),\n",
    "                                                                    entity_to_id=entity_mapping,\n",
    "                                                                    relation_to_id=relation_mapping\n",
    "                                                                    )\n",
    "    \n",
    "    datasets[key]['test'] = TriplesFactory.from_labeled_triples(df_test.astype('str').to_numpy(),\n",
    "                                                                    entity_to_id=entity_mapping,\n",
    "                                                                    relation_to_id=relation_mapping\n",
    "                                                                    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f04a673-f79c-42fd-9df9-2c2a34c2c5ea",
   "metadata": {
    "id": "6f04a673-f79c-42fd-9df9-2c2a34c2c5ea"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933ca0c9-396d-4849-8054-f3bcf5ee2151",
   "metadata": {
    "id": "933ca0c9-396d-4849-8054-f3bcf5ee2151"
   },
   "source": [
    "#### Defining models and their parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204d2f03-177f-4c36-ba0f-28c1ea7d007a",
   "metadata": {
    "id": "204d2f03-177f-4c36-ba0f-28c1ea7d007a"
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "          ('TransE', dict(scoring_fct_norm=2)),\n",
    "#          ('TransH', dict()),\n",
    "#          ('TransD', dict()),\n",
    "#          ('TransR', dict()),\n",
    "#          ('RESCAL', dict()),\n",
    "#          ('ComplEx', dict()),\n",
    "#          ('RotatE', dict())\n",
    "         ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rL9cGetPMPCf",
   "metadata": {
    "id": "rL9cGetPMPCf"
   },
   "source": [
    "Define parameters that are specific to a combination of dataset and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FlZRIRaNMOgs",
   "metadata": {
    "id": "FlZRIRaNMOgs"
   },
   "outputs": [],
   "source": [
    "class md_param:\n",
    "    def __init__(self, loss, training_loop):\n",
    "        self.loss = loss\n",
    "        self.training_loop = training_loop        \n",
    "\n",
    "\n",
    "model_dataset_param = {('TransE',  'wn18rr'):     md_param('BCEWithLogitsLoss', 'lcwa'),\n",
    "                       ('TransH',  'wn18rr'):     md_param('MarginRankingLoss', 'slcwa'),\n",
    "                       ('TransD',  'wn18rr'):     md_param('MarginRankingLoss', 'slcwa'),\n",
    "                       ('TransR',  'wn18rr'):     md_param('MarginRankingLoss', 'slcwa'),\n",
    "                       ('RESCAL',  'wn18rr'):     md_param('CrossEntropyLoss', 'lcwa'),\n",
    "                       ('ComplEx', 'wn18rr'):     md_param('CrossEntropyLoss', 'lcwa'),\n",
    "                       ('RotatE',  'wn18rr'):     md_param('BCEWithLogitsLoss', 'lcwa'),\n",
    "\n",
    "                       ('TransE',  'fb15k-237'):  md_param('MarginRankingLoss', 'slcwa'),\n",
    "                       ('TransH',  'fb15k-237'):  md_param('MarginRankingLoss', 'slcwa'),\n",
    "                       ('TransD',  'fb15k-237'):  md_param('MarginRankingLoss', 'slcwa'),\n",
    "                       ('TransR',  'fb15k-237'):  md_param('CrossEntropyLoss', 'lcwa'),\n",
    "                       ('RESCAL',  'fb15k-237'):  md_param('CrossEntropyLoss', 'lcwa'),                       \n",
    "                       ('ComplEx', 'fb15k-237'):  md_param('CrossEntropyLoss', 'lcwa'),\n",
    "                       ('RotatE',  'fb15k-237'):  md_param('NSSALoss', 'lcwa'),\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5TmfrEU8Dqf7",
   "metadata": {
    "id": "5TmfrEU8Dqf7"
   },
   "outputs": [],
   "source": [
    "from pykeen.hpo import hpo_pipeline\n",
    "    \n",
    "def run_hpo_pipeline(dataset_name, dataset, model):\n",
    "  model_name = model[0]\n",
    "  model_params = model[1]\n",
    "  dataset_base = str.join('-',str.split(dataset_name, '-')[:-1])\n",
    "  print(f\"Dataset: {dataset_name}, Model: {model_name}\")\n",
    "  result = hpo_pipeline(  \n",
    "    timeout=TIME_PER_TRIAL,\n",
    "    training=dataset['train'],\n",
    "    testing=dataset['test'],\n",
    "    validation=dataset['validate'],\n",
    "    model=model_name,\n",
    "    model_kwargs=model_params,\n",
    "    # set the parameters specific to this combination of dataset and model\n",
    "    loss=model_dataset_param[model_name, dataset_base].loss,\n",
    "    training_loop=model_dataset_param[model_name, dataset_base].training_loop,\n",
    "    training_kwargs=dict(num_epochs=N_EPOCHS,\n",
    "                         use_tqdm_batch=False),\n",
    "    training_kwargs_ranges=dict(\n",
    "                            batch_size=dict(type=int, low=256, high=1024, q=256),                    \n",
    "                            ),\n",
    "    device=CPU_DEV,\n",
    "    metric='MEAN_RECIPROCAL_RANK',\n",
    "    direction='maximize',\n",
    "    stopper='early',\n",
    "    stopper_kwargs=dict(frequency=10, patience=2, relative_delta=0.01),\n",
    "    model_kwargs_ranges=dict(\n",
    "                        embedding_dim=dict(type=int, low=32, high=400, q=64)\n",
    "                        )\n",
    "    ) \n",
    "\n",
    "  return(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b382dcd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4d67abbcba2644dd848a4dc59d440e10",
      "8248daea375847e4ae88814476a1b9a8",
      "9e465cc59c484e3f9c27e49eacd467ac",
      "567c948e45954066a4aa7a1a89098a42",
      "de412ff7cc664d7980f1b3e6599ccb29",
      "a266d7e9dab7475d9963e58b59618cf3",
      "d3650082397d47f5990c719da58ac988",
      "e439e21205ad402fb0b1cf03117049f8",
      "e7b0b0ee7807451f9bc34772b87d6dfe",
      "5c3e0eeabb28403b9d785f069fbff219",
      "49a44d4eeabe40ffb2f45d393a1ab44f",
      "953896fd4f474038943e2eb793cb381e",
      "a1d72432d1d74130a8245a4cbd7b3a34",
      "c5682eabe1ac4c0a98e57004e7d5e409",
      "67c18d106aaf4ae7baa9b230ae3fdd02",
      "027c176eb68c4f2dac2b2f3217e318ce",
      "830b475f509843afbdd708da9ea0cfe7",
      "1a1d8025f0a744988f99e393f5eb0e9d",
      "69379bacf88a442cb9c84a5a96879570",
      "2ed88daadce14e20ba6c448f7d938e76",
      "d9c7bc2a441b420e9a960d001f465b16",
      "bb564f5ffe4e40baac80b021a03c6c88",
      "bcf151eab4214bdf84dfcdfb4af91cb9",
      "bdf16783cbce4e548068fd46349baef5",
      "de343a9d40ea48baa0dd128601575eaa",
      "90a573216fd44732b9eb9cacf9268236",
      "b4b7a8209c2045b3acbd56f921d3d7c9",
      "186964fcc07e4d9fbbcad9095cff6ae6",
      "80668726066d4720b512a185de93ba41"
     ]
    },
    "id": "5b382dcd",
    "outputId": "510305c3-716c-43e4-9e86-4e95e86f815d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "for dataset_name, dataset in datasets.items():\n",
    "  print(dataset_name)\n",
    "\n",
    "  for model in models:\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    result = run_hpo_pipeline(dataset_name, dataset, model)\n",
    "    result.save_to_directory(f'{dataset_name}/{model[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "McGjcHbhS6JB",
   "metadata": {
    "id": "McGjcHbhS6JB"
   },
   "outputs": [],
   "source": [
    "result.save_to_directory(f'hpo_results/{dataset_name}/{model[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DKfR_rEn2x1G",
   "metadata": {
    "id": "DKfR_rEn2x1G"
   },
   "outputs": [],
   "source": [
    "def result_to_table(studies):\n",
    "\n",
    "  metrics = [('both.realistic.arithmetic_mean_rank', 'MR'),  \n",
    "            ('both.realistic.adjusted_arithmetic_mean_rank_index', 'AMRI'),\n",
    "            ('both.realistic.inverse_geometric_mean_rank', 'IGMR'),\n",
    "            ('both.realistic.hits_at_1', 'Hits@1'),\n",
    "            ('both.realistic.hits_at_3', 'Hits@3'),\n",
    "            ('both.realistic.hits_at_5', 'Hits@5')]\n",
    "\n",
    "  model_labels = [x[0] for x in models]           \n",
    "\n",
    "  for dataset in datasets.keys():\n",
    "    table = pd.DataFrame(columns = ['metric'] + model_labels)\n",
    "        \n",
    "    for metric in metrics:\n",
    "      row = [metric[1]]\n",
    "      for model_label in model_labels:        \n",
    "        result = studies[(dataset, model_label)].best_trial.user_attrs[metric[0]]\n",
    "        row += [round(result,5)]\n",
    "      table = table.append(pd.DataFrame([row], columns =['metric'] + model_labels ))\n",
    "\n",
    "    print(f'\\n\\nScores for the {dataset} dataset.\\n')\n",
    "    print(table.to_string(index=False))\n",
    "\n",
    "def print_best_params(model_name, study):        \n",
    "    print(f'\\nbest hyperparameters found for {model_name}')                        \n",
    "    print(pd.DataFrame.from_dict(study.best_params, orient='index').to_string())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Final Project - Embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
