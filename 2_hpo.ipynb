{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c40b4331",
   "metadata": {
    "id": "95564436-b5de-472d-9d76-8a6a05617be3"
   },
   "source": [
    "# Final project - Hyper parameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd04c126-6584-418f-a98e-3956f8d8f97b",
   "metadata": {
    "id": "bd04c126-6584-418f-a98e-3956f8d8f97b"
   },
   "source": [
    "## Installing packages (if running on colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebf26d2-6ad5-4b1e-ac81-7b695a276b16",
   "metadata": {
    "id": "1ebf26d2-6ad5-4b1e-ac81-7b695a276b16"
   },
   "source": [
    "**IMPORTANT**: Uncomment and run the cell below and then restart the Runtime (Menu Runtime > Restart Runtime, or with Ctrl + M .), then run it again. If you do not do that, then you will get errors. You only need to run it again if your Google Colab / Kaggle instance is restarted or lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92d7412a-4532-4bd6-b285-1a2dbe98ee91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92d7412a-4532-4bd6-b285-1a2dbe98ee91",
    "outputId": "45034e4d-b9b9-4a20-be77-3f91534c0910"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n! pip install --upgrade scipy\\n! pip install --upgrade pandas\\n! pip install ipywidgets\\n! pip uninstall -y pykeen\\n! pip install git+https://github.com/pykeen/pykeen.git@v1.5.0\\n! python -c \"import pykeen\" || pip install git+https://github.com/pykeen/pykeen.git@v1.5.0\\nfrom pkg_resources import require\\nrequire(\\'pykeen\\')\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "! pip install --upgrade scipy\n",
    "! pip install --upgrade pandas\n",
    "! pip install ipywidgets\n",
    "! pip uninstall -y pykeen\n",
    "! pip install git+https://github.com/pykeen/pykeen.git@v1.5.0\n",
    "! python -c \"import pykeen\" || pip install git+https://github.com/pykeen/pykeen.git@v1.5.0\n",
    "from pkg_resources import require\n",
    "require('pykeen')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zlvyH72mnCQZ",
   "metadata": {
    "id": "zlvyH72mnCQZ"
   },
   "source": [
    "After you install the packages above, you can just run from this cell onwards (Ctrl + F10 when this is selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83851aa",
   "metadata": {},
   "source": [
    "If SAVE_TO_DRIVE is set to True, the following cell enables storing of hpo results to your google drive account. Authentication required, and only works on colab instances (as far as we know)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bae5dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_TO_DRIVE = False\n",
    "if (SAVE_TO_DRIVE):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aa8990-f477-4544-8ea3-28eca654036c",
   "metadata": {
    "id": "22aa8990-f477-4544-8ea3-28eca654036c"
   },
   "source": [
    "## Imports and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d0e08e0-21fa-4c4e-94e7-65b113433dd1",
   "metadata": {
    "id": "2d0e08e0-21fa-4c4e-94e7-65b113433dd1"
   },
   "outputs": [],
   "source": [
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.triples import TriplesFactory\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "from pykeen.hpo import hpo_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c41521",
   "metadata": {},
   "source": [
    "Using the Local Closed World Assumption is computationally heavy compared to using the Stochastic Local Closed World Assumption. The selection of training loops impacts the choice of the ideal loss function as described in the article. The default setting in this notebook is to set SLCWA_ONLY = True, though it may be set to False to use the LCWA training loops for ideal optimization of the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42e63049",
   "metadata": {},
   "outputs": [],
   "source": [
    "SLCWA_ONLY = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6859571b-025a-45e7-9c46-e9794f45d03f",
   "metadata": {
    "id": "6859571b-025a-45e7-9c46-e9794f45d03f"
   },
   "source": [
    "How many epochs to use on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78042b10-fb4b-4b38-90a7-09b893995dc0",
   "metadata": {
    "id": "78042b10-fb4b-4b38-90a7-09b893995dc0"
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r-GxRe-eSiYB",
   "metadata": {
    "id": "r-GxRe-eSiYB"
   },
   "source": [
    "TIME_PER_TRIAL- How many seconds to use for each hyperparameter optimization trial. The optimizer will run as many trials as possible and stop after finishing the one that was runningwhen time ran out. *Not used, we run with n_trials instead*\n",
    "\n",
    "N_TRIALS - How many trials to run for each combination of dataset and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "niWdghjsSnkR",
   "metadata": {
    "id": "niWdghjsSnkR"
   },
   "outputs": [],
   "source": [
    "TIME_PER_TRIAL = 900\n",
    "N_TRIALS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cedb8c-a8d8-45fa-8a5f-a4a9103f6d54",
   "metadata": {
    "id": "27cedb8c-a8d8-45fa-8a5f-a4a9103f6d54"
   },
   "source": [
    "Always run on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0838dfb-8977-4557-a47a-5fff52a72e63",
   "metadata": {
    "id": "e0838dfb-8977-4557-a47a-5fff52a72e63"
   },
   "outputs": [],
   "source": [
    "CPU_DEV = 'gpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82df7457",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use this value for BASE_DATA_URL if working with local data\n",
    "BASE_DATA_URL = './data'\n",
    "\n",
    "# Use this value for BASE_DATA_URL if working with data from the github repo\n",
    "#BASE_DATA_URL = 'https://raw.githubusercontent.com/hvags/effective-octo-eureka/main/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67105167-2968-4965-bb4f-da544251a0e8",
   "metadata": {
    "id": "67105167-2968-4965-bb4f-da544251a0e8"
   },
   "source": [
    "## Retrieving data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb71192-0de3-4814-90dd-dbc927e3bb52",
   "metadata": {
    "id": "feb71192-0de3-4814-90dd-dbc927e3bb52"
   },
   "source": [
    "The data sets used were already pre stratified into training, testing and validation sets. Using the methods described in the data manipulation notebook they were split into symmetrical and asymmetrical subsets. The results were stored in a github repository for easy access in the further work.\n",
    "\n",
    "To retrieve the data we first populate a dictionary with the URL of the sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caac382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bac296",
   "metadata": {},
   "source": [
    "## wn18rr datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "532af1a6",
   "metadata": {
    "id": "532af1a6"
   },
   "outputs": [],
   "source": [
    "dataset_urls['wn18rr-full'] = {\n",
    "    'train': '/wn18rr/train_wn18rr.txt',\n",
    "    'validate': '/wn18rr/valid_wn18rr.txt',\n",
    "    'test': '/wn18rr/test_wn18rr.txt'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9da71dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls['wn18rr-sym'] = {\n",
    "    'train': '/wn18rr/sym_train_wn18rr.txt',\n",
    "    'validate': '/wn18rr/sym_valid_wn18rr.txt',\n",
    "    'test': '/wn18rr/sym_test_wn18rr.txt'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73128fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls['wn18rr-asym'] = {\n",
    "    'train': '/wn18rr/asym_train_wn18rr.txt',\n",
    "    'validate': '/wn18rr/asym_valid_wn18rr.txt',\n",
    "    'test': '/wn18rr/asym_test_wn18rr.txt'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf85bf59",
   "metadata": {},
   "source": [
    "## fb15k-237 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "x6HEzrD9Y3Py",
   "metadata": {
    "id": "x6HEzrD9Y3Py"
   },
   "outputs": [],
   "source": [
    "dataset_urls['fb15k-237-full'] = {\n",
    "    'train':  '/fb15k-237/train_fb15k-237.txt',\n",
    "    'validate': '/fb15k-237/valid_fb15k-237.txt',\n",
    "    'test': '/fb15k-237/test_fb15k-237.txt'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df37e284",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls['fb15k-237-sym'] = {\n",
    "    'train': '/fb15k-237/sym_train_fb15k-237.txt',\n",
    "    'validate': '/fb15k-237/sym_valid_fb15k-237.txt',\n",
    "    'test': '/fb15k-237/sym_test_fb15k-237.txt'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22ede1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls['fb15k-237-asym'] = {\n",
    "    'train': '/fb15k-237/asym_train_fb15k-237.txt',\n",
    "    'validate': '/fb15k-237/asym_valid_fb15k-237.txt',\n",
    "    'test': '/fb15k-237/asym_test_fb15k-237.txt'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c356e4f5",
   "metadata": {
    "id": "c356e4f5"
   },
   "source": [
    "## Read data from files and create TriplesFactories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fe2022a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7fe2022a",
    "outputId": "0ca75080-286e-4018-f247-353595bcf710"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: wn18rr-full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're trying to map triples with 211 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 210 from 3034 triples were filtered out\n",
      "You're trying to map triples with 212 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 210 from 3134 triples were filtered out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Processing: wn18rr-sym\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're trying to map triples with 60 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 48 from 1165 triples were filtered out\n",
      "You're trying to map triples with 35 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 31 from 1172 triples were filtered out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Processing: wn18rr-asym\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're trying to map triples with 527 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 518 from 1869 triples were filtered out\n",
      "You're trying to map triples with 555 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 546 from 1962 triples were filtered out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Processing: fb15k-237-full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're trying to map triples with 9 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 9 from 17535 triples were filtered out\n",
      "You're trying to map triples with 30 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 28 from 20466 triples were filtered out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Processing: fb15k-237-sym\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're trying to map triples with 192 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 110 from 524 triples were filtered out\n",
      "You're trying to map triples with 174 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 98 from 610 triples were filtered out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Processing: fb15k-237-asym\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're trying to map triples with 11 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 11 from 17011 triples were filtered out\n",
      "You're trying to map triples with 30 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 28 from 19856 triples were filtered out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = dict()\n",
    "\n",
    "for key in dataset_urls.keys():\n",
    "    print(f'Processing: {key}')\n",
    "    \n",
    "    datasets[key] = dict()\n",
    "    \n",
    "    df_train = pd.read_csv(BASE_DATA_URL + dataset_urls[key]['train'], header=None, sep='\\t', names=['head', 'relation','tail'])\n",
    "    df_validate = pd.read_csv(BASE_DATA_URL + dataset_urls[key]['validate'], header=None, sep='\\t', names=['head', 'relation','tail'])\n",
    "    df_test = pd.read_csv(BASE_DATA_URL + dataset_urls[key]['test'], header=None, sep='\\t', names=['head', 'relation','tail'])\n",
    "    \n",
    "    datasets[key]['train'] = TriplesFactory.from_labeled_triples(df_train.astype('str').to_numpy())\n",
    "    entity_mapping = datasets[key]['train'].entity_to_id\n",
    "    relation_mapping = datasets[key]['train'].relation_to_id\n",
    "    \n",
    "    datasets[key]['validate'] = TriplesFactory.from_labeled_triples(df_validate.astype('str').to_numpy(),\n",
    "                                                                    entity_to_id=entity_mapping,\n",
    "                                                                    relation_to_id=relation_mapping\n",
    "                                                                    )\n",
    "    \n",
    "    datasets[key]['test'] = TriplesFactory.from_labeled_triples(df_test.astype('str').to_numpy(),\n",
    "                                                                    entity_to_id=entity_mapping,\n",
    "                                                                    relation_to_id=relation_mapping\n",
    "                                                                    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f04a673-f79c-42fd-9df9-2c2a34c2c5ea",
   "metadata": {
    "id": "6f04a673-f79c-42fd-9df9-2c2a34c2c5ea"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933ca0c9-396d-4849-8054-f3bcf5ee2151",
   "metadata": {
    "id": "933ca0c9-396d-4849-8054-f3bcf5ee2151"
   },
   "source": [
    "#### Defining models and their parameters\n",
    "If doing a partial run, comment out models that should not be included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "204d2f03-177f-4c36-ba0f-28c1ea7d007a",
   "metadata": {
    "id": "204d2f03-177f-4c36-ba0f-28c1ea7d007a"
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "          #('TransE', dict(scoring_fct_norm=2)),\n",
    "          #('TransH', dict()),\n",
    "          #('TransD', dict()),\n",
    "          #('TransR', dict()),\n",
    "          ('RESCAL', dict()),\n",
    "          #('ComplEx', dict()),\n",
    "          #('RotatE', dict())\n",
    "         ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rL9cGetPMPCf",
   "metadata": {
    "id": "rL9cGetPMPCf"
   },
   "source": [
    "Define parameters that are specific to a combination of dataset and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "FlZRIRaNMOgs",
   "metadata": {
    "id": "FlZRIRaNMOgs"
   },
   "outputs": [],
   "source": [
    "class md_param:\n",
    "    def __init__(self, loss, training_loop):\n",
    "        self.loss = loss\n",
    "        self.training_loop = training_loop        \n",
    "\n",
    "        \n",
    "if (SLCWA_ONLY == False):\n",
    "    model_dataset_param = {('TransE',  'wn18rr'):     md_param('BCEWithLogitsLoss', 'lcwa'),\n",
    "                           ('TransH',  'wn18rr'):     md_param('MarginRankingLoss', 'slcwa'),\n",
    "                           ('TransD',  'wn18rr'):     md_param('MarginRankingLoss', 'slcwa'),\n",
    "                           ('TransR',  'wn18rr'):     md_param('MarginRankingLoss', 'slcwa'),                       \n",
    "                           ('RESCAL',  'wn18rr'):     md_param('CrossEntropyLoss', 'lcwa'),                                              \n",
    "                           ('ComplEx', 'wn18rr'):     md_param('CrossEntropyLoss', 'lcwa'),\n",
    "                           ('RotatE',  'wn18rr'):     md_param('BCEWithLogitsLoss', 'lcwa'),\n",
    "\n",
    "                           ('TransE',  'fb15k-237'):  md_param('MarginRankingLoss', 'slcwa'),\n",
    "                           ('TransH',  'fb15k-237'):  md_param('MarginRankingLoss', 'slcwa'),\n",
    "                           ('TransD',  'fb15k-237'):  md_param('MarginRankingLoss', 'slcwa'),\n",
    "                           ('TransR',  'fb15k-237'):  md_param('CrossEntropyLoss', 'lcwa'),                                              \n",
    "                           ('RESCAL',  'fb15k-237'):  md_param('CrossEntropyLoss', 'lcwa'),                        \n",
    "                           ('ComplEx', 'fb15k-237'):  md_param('CrossEntropyLoss', 'lcwa'),\n",
    "                           ('RotatE',  'fb15k-237'):  md_param('NSSALoss', 'lcwa'),\n",
    "                          }\n",
    "\n",
    "else:\n",
    "    model_dataset_param = {('TransE',  'wn18rr'):     md_param('NSSALoss', 'slcwa'),\n",
    "                           ('TransH',  'wn18rr'):     md_param('MarginRankingLoss', 'slcwa'),\n",
    "                           ('TransD',  'wn18rr'):     md_param('MarginRankingLoss', 'slcwa'),\n",
    "                           ('TransR',  'wn18rr'):     md_param('MarginRankingLoss', 'slcwa'),                       \n",
    "                           ('RESCAL',  'wn18rr'):     md_param('NSSALoss', 'slcwa'),                                              \n",
    "                           ('ComplEx', 'wn18rr'):     md_param('BCEWithLogitsLoss', 'slcwa'),\n",
    "                           ('RotatE',  'wn18rr'):     md_param('NSSALoss', 'slcwa'),\n",
    "\n",
    "                           ('TransE',  'fb15k-237'):  md_param('MarginRankingLoss', 'slcwa'),\n",
    "                           ('TransH',  'fb15k-237'):  md_param('MarginRankingLoss', 'slcwa'),\n",
    "                           ('TransD',  'fb15k-237'):  md_param('MarginRankingLoss', 'slcwa'),\n",
    "                           ('TransR',  'fb15k-237'):  md_param('MarginRankingLoss', 'slcwa'),                                              \n",
    "                           ('RESCAL',  'fb15k-237'):  md_param('MarginRankingLoss', 'slcwa'),                        \n",
    "                           ('ComplEx', 'fb15k-237'):  md_param('BCEWithLogitsLoss', 'slcwa'),\n",
    "                           ('RotatE',  'fb15k-237'):  md_param('NSSALoss', 'slcwa'),\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5TmfrEU8Dqf7",
   "metadata": {
    "id": "5TmfrEU8Dqf7"
   },
   "outputs": [],
   "source": [
    "def run_hpo_pipeline(dataset_name, dataset, model):\n",
    "  model_name = model[0]\n",
    "  model_params = model[1]\n",
    "  dataset_base = str.join('-',str.split(dataset_name, '-')[:-1])\n",
    "  print(f\"Dataset: {dataset_name}, Model: {model_name}\")\n",
    "  result = hpo_pipeline(  \n",
    "    n_trials=N_TRIALS,  \n",
    "    training=dataset['train'],\n",
    "    testing=dataset['test'],\n",
    "    validation=dataset['validate'],\n",
    "    model=model_name,\n",
    "    model_kwargs=model_params,\n",
    "    # set the parameters specific to this combination of dataset and model\n",
    "    loss=model_dataset_param[model_name, dataset_base].loss,\n",
    "    training_loop=model_dataset_param[model_name, dataset_base].training_loop,\n",
    "    training_kwargs=dict(num_epochs=N_EPOCHS,\n",
    "                         use_tqdm_batch=False),\n",
    "    training_kwargs_ranges=dict(\n",
    "                            batch_size=dict(type=int, low=256, high=1024, q=256),                    \n",
    "                            ),\n",
    "    device=CPU_DEV,\n",
    "    metric='MEAN_RECIPROCAL_RANK',\n",
    "    direction='maximize',\n",
    "    stopper='early',\n",
    "    stopper_kwargs=dict(frequency=10, patience=2, relative_delta=0.01),\n",
    "    model_kwargs_ranges=dict(\n",
    "                        embedding_dim=dict(type=int, low=32, high=400, q=64)\n",
    "                        )\n",
    "    ) \n",
    "\n",
    "  return(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b382dcd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4d67abbcba2644dd848a4dc59d440e10",
      "8248daea375847e4ae88814476a1b9a8",
      "9e465cc59c484e3f9c27e49eacd467ac",
      "567c948e45954066a4aa7a1a89098a42",
      "de412ff7cc664d7980f1b3e6599ccb29",
      "a266d7e9dab7475d9963e58b59618cf3",
      "d3650082397d47f5990c719da58ac988",
      "e439e21205ad402fb0b1cf03117049f8",
      "e7b0b0ee7807451f9bc34772b87d6dfe",
      "5c3e0eeabb28403b9d785f069fbff219",
      "49a44d4eeabe40ffb2f45d393a1ab44f",
      "953896fd4f474038943e2eb793cb381e",
      "a1d72432d1d74130a8245a4cbd7b3a34",
      "c5682eabe1ac4c0a98e57004e7d5e409",
      "67c18d106aaf4ae7baa9b230ae3fdd02",
      "027c176eb68c4f2dac2b2f3217e318ce",
      "830b475f509843afbdd708da9ea0cfe7",
      "1a1d8025f0a744988f99e393f5eb0e9d",
      "69379bacf88a442cb9c84a5a96879570",
      "2ed88daadce14e20ba6c448f7d938e76",
      "d9c7bc2a441b420e9a960d001f465b16",
      "bb564f5ffe4e40baac80b021a03c6c88",
      "bcf151eab4214bdf84dfcdfb4af91cb9",
      "bdf16783cbce4e548068fd46349baef5",
      "de343a9d40ea48baa0dd128601575eaa",
      "90a573216fd44732b9eb9cacf9268236",
      "b4b7a8209c2045b3acbd56f921d3d7c9",
      "186964fcc07e4d9fbbcad9095cff6ae6",
      "80668726066d4720b512a185de93ba41"
     ]
    },
    "id": "5b382dcd",
    "outputId": "510305c3-716c-43e4-9e86-4e95e86f815d",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-11-11 11:22:42,765]\u001b[0m A new study created in memory with name: no-name-82b372bc-2dff-469d-8081-d9d8ec1105ef\u001b[0m\n",
      "C:\\Appl\\Anaconda3\\envs\\cuda\\lib\\site-packages\\optuna\\distributions.py:560: UserWarning: The distribution is specified by [32, 400] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 352].\n",
      "  warnings.warn(\n",
      "No random seed is specified. Setting to 2839519528.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wn18rr-full\n",
      "Dataset: wn18rr-full, Model: RESCAL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c6c580b5d94787904abd0299432b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs on cuda:   0%|          | 0/100 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.evaluation.evaluator:Starting batch_size search for evaluation now...\n",
      "INFO:pykeen.evaluation.evaluator:Concluded batch_size search with batch_size=64.\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 16.25s seconds\n",
      "INFO:pykeen.training.training_loop:=> Saved checkpoint after having finished epoch 10.\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 16.07s seconds\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 16.04s seconds\n",
      "INFO:pykeen.stoppers.early_stopping:Stopping early after 3 evaluations at epoch 30. The best result hits_at_k=0.00017705382436260624 occurred at epoch 10.\n",
      "INFO:pykeen.training.training_loop:=> loading checkpoint 'C:\\Users\\hvags\\AppData\\Local\\Temp\\tmpqkd61l04'\n",
      "INFO:pykeen.training.training_loop:=> loaded checkpoint 'C:\\Users\\hvags\\AppData\\Local\\Temp\\tmpqkd61l04' stopped after having finished epoch 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45fa1f36dfa4cf88118394c4e8f983b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating on cuda:   0%|          | 0.00/2.82k [00:00<?, ?triple/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.evaluation.evaluator:Evaluation took 16.11s seconds\n",
      "\u001b[32m[I 2021-11-11 11:27:54,287]\u001b[0m Trial 0 finished with value: 0.00019207599000791857 and parameters: {'model.embedding_dim': 160, 'loss.margin': 18, 'loss.adversarial_temperature': 0.8599550830787281, 'regularizer.weight': 0.4999898154211292, 'optimizer.lr': 0.03126477281996058, 'negative_sampler.num_negs_per_pos': 1, 'training.batch_size': 768}. Best is trial 0 with value: 0.00019207599000791857.\u001b[0m\n",
      "C:\\Appl\\Anaconda3\\envs\\cuda\\lib\\site-packages\\optuna\\distributions.py:560: UserWarning: The distribution is specified by [32, 400] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 352].\n",
      "  warnings.warn(\n",
      "WARNING:pykeen.pipeline.api:No random seed is specified. Setting to 2389547490.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6362818df8f4f0ebb653a6da3586651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs on cuda:   0%|          | 0/100 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.evaluation.evaluator:Starting batch_size search for evaluation now...\n",
      "INFO:pykeen.evaluation.evaluator:Concluded batch_size search with batch_size=64.\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 16.04s seconds\n",
      "INFO:pykeen.training.training_loop:=> Saved checkpoint after having finished epoch 10.\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 16.36s seconds\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 16.13s seconds\n",
      "INFO:pykeen.training.training_loop:=> Saved checkpoint after having finished epoch 30.\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 16.24s seconds\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 16.25s seconds\n",
      "INFO:pykeen.stoppers.early_stopping:Stopping early after 5 evaluations at epoch 50. The best result hits_at_k=0.00017705382436260624 occurred at epoch 30.\n",
      "INFO:pykeen.training.training_loop:=> loading checkpoint 'C:\\Users\\hvags\\AppData\\Local\\Temp\\tmp2_dj4u10'\n",
      "INFO:pykeen.training.training_loop:=> loaded checkpoint 'C:\\Users\\hvags\\AppData\\Local\\Temp\\tmp2_dj4u10' stopped after having finished epoch 30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c334f8d3e56e444c86b0d428f04c86a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating on cuda:   0%|          | 0.00/2.82k [00:00<?, ?triple/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.evaluation.evaluator:Evaluation took 16.29s seconds\n",
      "\u001b[32m[I 2021-11-11 11:38:51,518]\u001b[0m Trial 1 finished with value: 0.0001804700743885965 and parameters: {'model.embedding_dim': 160, 'loss.margin': 9, 'loss.adversarial_temperature': 0.9079065523901304, 'regularizer.weight': 0.996935347165849, 'optimizer.lr': 0.02051035090356538, 'negative_sampler.num_negs_per_pos': 2, 'training.batch_size': 768}. Best is trial 0 with value: 0.00019207599000791857.\u001b[0m\n",
      "C:\\Appl\\Anaconda3\\envs\\cuda\\lib\\site-packages\\optuna\\distributions.py:560: UserWarning: The distribution is specified by [32, 400] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 352].\n",
      "  warnings.warn(\n",
      "WARNING:pykeen.pipeline.api:No random seed is specified. Setting to 3566218222.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64abf1cc838a4e9599341870914e763d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs on cuda:   0%|          | 0/100 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.evaluation.evaluator:Starting batch_size search for evaluation now...\n",
      "INFO:pykeen.evaluation.evaluator:Concluded batch_size search with batch_size=64.\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 25.00s seconds\n",
      "INFO:pykeen.training.training_loop:=> Saved checkpoint after having finished epoch 10.\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 24.86s seconds\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 25.27s seconds\n",
      "INFO:pykeen.stoppers.early_stopping:Stopping early after 3 evaluations at epoch 30. The best result hits_at_k=0.07790368271954674 occurred at epoch 10.\n",
      "INFO:pykeen.training.training_loop:=> loading checkpoint 'C:\\Users\\hvags\\AppData\\Local\\Temp\\tmpn0ft9vm5'\n",
      "INFO:pykeen.training.training_loop:=> loaded checkpoint 'C:\\Users\\hvags\\AppData\\Local\\Temp\\tmpn0ft9vm5' stopped after having finished epoch 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97def87a695a4bf9b25473bb3ef9ec0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating on cuda:   0%|          | 0.00/2.82k [00:00<?, ?triple/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.evaluation.evaluator:Evaluation took 24.95s seconds\n",
      "\u001b[32m[I 2021-11-11 11:52:21,740]\u001b[0m Trial 2 finished with value: 0.04634231809420707 and parameters: {'model.embedding_dim': 224, 'loss.margin': 24, 'loss.adversarial_temperature': 0.6911642038233735, 'regularizer.weight': 0.13538893138472385, 'optimizer.lr': 0.06647293940143989, 'negative_sampler.num_negs_per_pos': 2, 'training.batch_size': 256}. Best is trial 2 with value: 0.04634231809420707.\u001b[0m\n",
      "C:\\Appl\\Anaconda3\\envs\\cuda\\lib\\site-packages\\optuna\\distributions.py:560: UserWarning: The distribution is specified by [32, 400] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 352].\n",
      "  warnings.warn(\n",
      "WARNING:pykeen.pipeline.api:No random seed is specified. Setting to 2947445030.\n",
      "INFO:pykeen.training.training_loop:Starting sub_batch_size search for training now...\n",
      "INFO:pykeen.training.training_loop:Concluded search with sub_batch_size 512.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c15092c776410b94ea8df247595c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs on cuda:   0%|          | 0/100 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.evaluation.evaluator:Starting batch_size search for evaluation now...\n",
      "INFO:pykeen.evaluation.evaluator:Concluded batch_size search with batch_size=128.\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 6.98s seconds\n",
      "INFO:pykeen.training.training_loop:=> Saved checkpoint after having finished epoch 10.\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 7.06s seconds\n",
      "INFO:pykeen.training.training_loop:=> Saved checkpoint after having finished epoch 20.\n",
      "WARNING:pykeen.training.training_loop:The training loop just failed during epoch 30 due to error CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 1.92 GiB already allocated; 0 bytes free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF.\n",
      "\u001b[33m[W 2021-11-11 12:08:12,544]\u001b[0m Trial 3 failed, because the value None could not be cast to float.\u001b[0m\n",
      "C:\\Appl\\Anaconda3\\envs\\cuda\\lib\\site-packages\\optuna\\distributions.py:560: UserWarning: The distribution is specified by [32, 400] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 352].\n",
      "  warnings.warn(\n",
      "WARNING:pykeen.pipeline.api:No random seed is specified. Setting to 4025888093.\n",
      "INFO:pykeen.training.training_loop:Starting sub_batch_size search for training now...\n",
      "INFO:pykeen.training.training_loop:Concluded search with sub_batch_size 384.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e7088ba1c045b785a460a811d073a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs on cuda:   0%|          | 0/100 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.evaluation.evaluator:Starting batch_size search for evaluation now...\n",
      "INFO:pykeen.evaluation.evaluator:Concluded batch_size search with batch_size=32.\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 37.79s seconds\n",
      "INFO:pykeen.training.training_loop:=> Saved checkpoint after having finished epoch 10.\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 37.75s seconds\n",
      "INFO:pykeen.training.training_loop:=> Saved checkpoint after having finished epoch 20.\n",
      "WARNING:pykeen.training.training_loop:The training loop just failed during epoch 30 due to error CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 4.00 GiB total capacity; 197.08 MiB already allocated; 0 bytes free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF.\n",
      "\u001b[33m[W 2021-11-11 12:35:58,557]\u001b[0m Trial 4 failed, because the value None could not be cast to float.\u001b[0m\n",
      "C:\\Appl\\Anaconda3\\envs\\cuda\\lib\\site-packages\\optuna\\distributions.py:560: UserWarning: The distribution is specified by [32, 400] and step=64, but the range is not divisible by `step`. It will be replaced by [32, 352].\n",
      "  warnings.warn(\n",
      "WARNING:pykeen.pipeline.api:No random seed is specified. Setting to 343070142.\n",
      "INFO:pykeen.training.training_loop:Starting sub_batch_size search for training now...\n",
      "INFO:pykeen.training.training_loop:Concluded search with sub_batch_size 32.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c33aad54344fd1affbc372f818b32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs on cuda:   0%|          | 0/100 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for dataset_name, dataset in datasets.items():\n",
    "  print(dataset_name)\n",
    "\n",
    "  for model in models:\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    result = run_hpo_pipeline(dataset_name, dataset, model)\n",
    "    result.save_to_directory(f'hpo_results/{dataset_name}/{model[0]}')\n",
    "    if (SAVE_TO_DRIVE):\n",
    "        result.save_to_directory(f'/content/drive/MyDrive/hpo_results/{dataset_name}/{model[0]}')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Final Project - Embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
