{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c40b4331",
   "metadata": {
    "id": "95564436-b5de-472d-9d76-8a6a05617be3"
   },
   "source": [
    "# Final project - Hyper parameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd04c126-6584-418f-a98e-3956f8d8f97b",
   "metadata": {
    "id": "bd04c126-6584-418f-a98e-3956f8d8f97b"
   },
   "source": [
    "## Installing packages (if running on colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebf26d2-6ad5-4b1e-ac81-7b695a276b16",
   "metadata": {
    "id": "1ebf26d2-6ad5-4b1e-ac81-7b695a276b16"
   },
   "source": [
    "**IMPORTANT**: Uncomment and run the cell below and then restart the Runtime (Menu Runtime > Restart Runtime, or with Ctrl + M .), then run it again. If you do not do that, then you will get errors. You only need to run it again if your Google Colab / Kaggle instance is restarted or lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d7412a-4532-4bd6-b285-1a2dbe98ee91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92d7412a-4532-4bd6-b285-1a2dbe98ee91",
    "outputId": "45034e4d-b9b9-4a20-be77-3f91534c0910"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "! pip install --upgrade scipy\n",
    "! pip install --upgrade pandas\n",
    "! pip install ipywidgets\n",
    "! pip uninstall -y pykeen\n",
    "! pip install git+https://github.com/pykeen/pykeen.git@v1.5.0\n",
    "! python -c \"import pykeen\" || pip install git+https://github.com/pykeen/pykeen.git@v1.5.0\n",
    "from pkg_resources import require\n",
    "require('pykeen')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zlvyH72mnCQZ",
   "metadata": {
    "id": "zlvyH72mnCQZ"
   },
   "source": [
    "After you install the packages above, you can just run from this cell onwards (Ctrl + F10 when this is selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f5f194",
   "metadata": {},
   "source": [
    "If SAVE_TO_DRIVE is set to True, the following cell enables storing of hpo results to your google drive account. Authentication required, and only works on colab instances (as far as we know)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73a16e2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7364/2392536937.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mSAVE_TO_DRIVE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSAVE_TO_DRIVE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "SAVE_TO_DRIVE = False\n",
    "if (SAVE_TO_DRIVE):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aa8990-f477-4544-8ea3-28eca654036c",
   "metadata": {
    "id": "22aa8990-f477-4544-8ea3-28eca654036c"
   },
   "source": [
    "## Imports and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0e08e0-21fa-4c4e-94e7-65b113433dd1",
   "metadata": {
    "id": "2d0e08e0-21fa-4c4e-94e7-65b113433dd1"
   },
   "outputs": [],
   "source": [
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.triples import TriplesFactory\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "from pykeen.hpo import hpo_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6859571b-025a-45e7-9c46-e9794f45d03f",
   "metadata": {
    "id": "6859571b-025a-45e7-9c46-e9794f45d03f"
   },
   "source": [
    "How many epochs to use on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78042b10-fb4b-4b38-90a7-09b893995dc0",
   "metadata": {
    "id": "78042b10-fb4b-4b38-90a7-09b893995dc0"
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r-GxRe-eSiYB",
   "metadata": {
    "id": "r-GxRe-eSiYB"
   },
   "source": [
    "How many seconds to use for each hyperparameter optimization trial. The optimizer will run as many trials as possible and stop after finishing the one that was runningwhen time ran out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "niWdghjsSnkR",
   "metadata": {
    "id": "niWdghjsSnkR"
   },
   "outputs": [],
   "source": [
    "TIME_PER_TRIAL = 900"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cedb8c-a8d8-45fa-8a5f-a4a9103f6d54",
   "metadata": {
    "id": "27cedb8c-a8d8-45fa-8a5f-a4a9103f6d54"
   },
   "source": [
    "Always run on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0838dfb-8977-4557-a47a-5fff52a72e63",
   "metadata": {
    "id": "e0838dfb-8977-4557-a47a-5fff52a72e63"
   },
   "outputs": [],
   "source": [
    "CPU_DEV = 'gpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82df7457",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use this value for BASE_DATA_URL if working with local data\n",
    "BASE_DATA_URL = './data'\n",
    "\n",
    "# Use this value for BASE_DATA_URL if working with data from the github repo\n",
    "#BASE_DATA_URL = 'https://raw.githubusercontent.com/hvags/effective-octo-eureka/main/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67105167-2968-4965-bb4f-da544251a0e8",
   "metadata": {
    "id": "67105167-2968-4965-bb4f-da544251a0e8"
   },
   "source": [
    "## Retrieving data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb71192-0de3-4814-90dd-dbc927e3bb52",
   "metadata": {
    "id": "feb71192-0de3-4814-90dd-dbc927e3bb52"
   },
   "source": [
    "The data sets used were already pre stratified into training, testing and validation sets. Using the methods described in the data manipulation notebook they were split into symmetrical and asymmetrical subsets. The results were stored in a github repository for easy access in the further work.\n",
    "\n",
    "To retrieve the data we first populate a dictionary with the URL of the sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caac382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bac296",
   "metadata": {},
   "source": [
    "## wn18rr datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532af1a6",
   "metadata": {
    "id": "532af1a6"
   },
   "outputs": [],
   "source": [
    "dataset_urls['wn18rr-full'] = {\n",
    "    'train': '/wn18rr/train_wn18rr.txt',\n",
    "    'validate': '/wn18rr/valid_wn18rr.txt',\n",
    "    'test': '/wn18rr/test_wn18rr.txt'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da71dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls['wn18rr-sym'] = {\n",
    "    'train': '/wn18rr/sym_train_wn18rr.txt',\n",
    "    'validate': '/wn18rr/sym_valid_wn18rr.txt',\n",
    "    'test': '/wn18rr/sym_test_wn18rr.txt'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73128fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls['wn18rr-asym'] = {\n",
    "    'train': '/wn18rr/asym_train_wn18rr.txt',\n",
    "    'validate': '/wn18rr/asym_valid_wn18rr.txt',\n",
    "    'test': '/wn18rr/asym_test_wn18rr.txt'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf85bf59",
   "metadata": {},
   "source": [
    "## fb15k-237 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x6HEzrD9Y3Py",
   "metadata": {
    "id": "x6HEzrD9Y3Py"
   },
   "outputs": [],
   "source": [
    "dataset_urls['fb15k-237-full'] = {\n",
    "    'train':  '/fb15k-237/train_fb15k-237.txt',\n",
    "    'validate': '/fb15k-237/valid_fb15k-237.txt',\n",
    "    'test': '/fb15k-237/test_fb15k-237.txt'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df37e284",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls['fb15k-237-sym'] = {\n",
    "    'train': '/fb15k-237/sym_train_fb15k-237.txt',\n",
    "    'validate': '/fb15k-237/sym_valid_fb15k-237.txt',\n",
    "    'test': '/fb15k-237/sym_test_fb15k-237.txt'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ede1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls['fb15k-237-asym'] = {\n",
    "    'train': '/fb15k-237/asym_train_fb15k-237.txt',\n",
    "    'validate': '/fb15k-237/asym_valid_fb15k-237.txt',\n",
    "    'test': '/fb15k-237/asym_test_fb15k-237.txt'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c356e4f5",
   "metadata": {
    "id": "c356e4f5"
   },
   "source": [
    "## Read data from files and create TriplesFactories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe2022a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7fe2022a",
    "outputId": "0ca75080-286e-4018-f247-353595bcf710"
   },
   "outputs": [],
   "source": [
    "datasets = dict()\n",
    "\n",
    "for key in dataset_urls.keys():\n",
    "    print(f'Processing: {key}')\n",
    "    \n",
    "    datasets[key] = dict()\n",
    "    \n",
    "    df_train = pd.read_csv(BASE_DATA_URL + dataset_urls[key]['train'], header=None, sep='\\t', names=['head', 'relation','tail'])\n",
    "    df_validate = pd.read_csv(BASE_DATA_URL + dataset_urls[key]['validate'], header=None, sep='\\t', names=['head', 'relation','tail'])\n",
    "    df_test = pd.read_csv(BASE_DATA_URL + dataset_urls[key]['test'], header=None, sep='\\t', names=['head', 'relation','tail'])\n",
    "    \n",
    "    datasets[key]['train'] = TriplesFactory.from_labeled_triples(df_train.astype('str').to_numpy())\n",
    "    entity_mapping = datasets[key]['train'].entity_to_id\n",
    "    relation_mapping = datasets[key]['train'].relation_to_id\n",
    "    \n",
    "    datasets[key]['validate'] = TriplesFactory.from_labeled_triples(df_validate.astype('str').to_numpy(),\n",
    "                                                                    entity_to_id=entity_mapping,\n",
    "                                                                    relation_to_id=relation_mapping\n",
    "                                                                    )\n",
    "    \n",
    "    datasets[key]['test'] = TriplesFactory.from_labeled_triples(df_test.astype('str').to_numpy(),\n",
    "                                                                    entity_to_id=entity_mapping,\n",
    "                                                                    relation_to_id=relation_mapping\n",
    "                                                                    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f04a673-f79c-42fd-9df9-2c2a34c2c5ea",
   "metadata": {
    "id": "6f04a673-f79c-42fd-9df9-2c2a34c2c5ea"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933ca0c9-396d-4849-8054-f3bcf5ee2151",
   "metadata": {
    "id": "933ca0c9-396d-4849-8054-f3bcf5ee2151"
   },
   "source": [
    "#### Defining models and their parameters\n",
    "If doing a partial run, comment out models that should not be included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204d2f03-177f-4c36-ba0f-28c1ea7d007a",
   "metadata": {
    "id": "204d2f03-177f-4c36-ba0f-28c1ea7d007a"
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "          ('TransE', dict(scoring_fct_norm=2)),\n",
    "          ('TransH', dict()),\n",
    "          ('TransD', dict()),\n",
    "          ('TransR', dict()),\n",
    "          ('RESCAL', dict()),\n",
    "          ('ComplEx', dict()),\n",
    "          ('RotatE', dict())\n",
    "         ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rL9cGetPMPCf",
   "metadata": {
    "id": "rL9cGetPMPCf"
   },
   "source": [
    "Define parameters that are specific to a combination of dataset and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FlZRIRaNMOgs",
   "metadata": {
    "id": "FlZRIRaNMOgs"
   },
   "outputs": [],
   "source": [
    "class md_param:\n",
    "    def __init__(self, loss, training_loop):\n",
    "        self.loss = loss\n",
    "        self.training_loop = training_loop        \n",
    "\n",
    "\n",
    "model_dataset_param = {('TransE',  'wn18rr'):     md_param('BCEWithLogitsLoss', 'lcwa'),\n",
    "                       ('TransH',  'wn18rr'):     md_param('MarginRankingLoss', 'slcwa'),\n",
    "                       ('TransD',  'wn18rr'):     md_param('MarginRankingLoss', 'slcwa'),\n",
    "                       ('TransR',  'wn18rr'):     md_param('MarginRankingLoss', 'slcwa'),\n",
    "                       ('RESCAL',  'wn18rr'):     md_param('CrossEntropyLoss', 'lcwa'),\n",
    "                       ('ComplEx', 'wn18rr'):     md_param('CrossEntropyLoss', 'lcwa'),\n",
    "                       ('RotatE',  'wn18rr'):     md_param('BCEWithLogitsLoss', 'lcwa'),\n",
    "\n",
    "                       ('TransE',  'fb15k-237'):  md_param('MarginRankingLoss', 'slcwa'),\n",
    "                       ('TransH',  'fb15k-237'):  md_param('MarginRankingLoss', 'slcwa'),\n",
    "                       ('TransD',  'fb15k-237'):  md_param('MarginRankingLoss', 'slcwa'),\n",
    "                       ('TransR',  'fb15k-237'):  md_param('CrossEntropyLoss', 'lcwa'),\n",
    "                       ('RESCAL',  'fb15k-237'):  md_param('CrossEntropyLoss', 'lcwa'),                       \n",
    "                       ('ComplEx', 'fb15k-237'):  md_param('CrossEntropyLoss', 'lcwa'),\n",
    "                       ('RotatE',  'fb15k-237'):  md_param('NSSALoss', 'lcwa'),\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5TmfrEU8Dqf7",
   "metadata": {
    "id": "5TmfrEU8Dqf7"
   },
   "outputs": [],
   "source": [
    "def run_hpo_pipeline(dataset_name, dataset, model):\n",
    "  model_name = model[0]\n",
    "  model_params = model[1]\n",
    "  dataset_base = str.join('-',str.split(dataset_name, '-')[:-1])\n",
    "  print(f\"Dataset: {dataset_name}, Model: {model_name}\")\n",
    "  result = hpo_pipeline(  \n",
    "    n_trials=30,\n",
    "    timeout=TIME_PER_TRIAL,\n",
    "    training=dataset['train'],\n",
    "    testing=dataset['test'],\n",
    "    validation=dataset['validate'],\n",
    "    model=model_name,\n",
    "    model_kwargs=model_params,\n",
    "    # set the parameters specific to this combination of dataset and model\n",
    "    loss=model_dataset_param[model_name, dataset_base].loss,\n",
    "    training_loop=model_dataset_param[model_name, dataset_base].training_loop,\n",
    "    training_kwargs=dict(num_epochs=N_EPOCHS,\n",
    "                         use_tqdm_batch=False),\n",
    "    training_kwargs_ranges=dict(\n",
    "                            batch_size=dict(type=int, low=256, high=1024, q=256),                    \n",
    "                            ),\n",
    "    device=CPU_DEV,\n",
    "    metric='MEAN_RECIPROCAL_RANK',\n",
    "    direction='maximize',\n",
    "    stopper='early',\n",
    "    stopper_kwargs=dict(frequency=10, patience=2, relative_delta=0.01),\n",
    "    model_kwargs_ranges=dict(\n",
    "                        embedding_dim=dict(type=int, low=32, high=400, q=64)\n",
    "                        )\n",
    "    ) \n",
    "\n",
    "  return(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b382dcd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4d67abbcba2644dd848a4dc59d440e10",
      "8248daea375847e4ae88814476a1b9a8",
      "9e465cc59c484e3f9c27e49eacd467ac",
      "567c948e45954066a4aa7a1a89098a42",
      "de412ff7cc664d7980f1b3e6599ccb29",
      "a266d7e9dab7475d9963e58b59618cf3",
      "d3650082397d47f5990c719da58ac988",
      "e439e21205ad402fb0b1cf03117049f8",
      "e7b0b0ee7807451f9bc34772b87d6dfe",
      "5c3e0eeabb28403b9d785f069fbff219",
      "49a44d4eeabe40ffb2f45d393a1ab44f",
      "953896fd4f474038943e2eb793cb381e",
      "a1d72432d1d74130a8245a4cbd7b3a34",
      "c5682eabe1ac4c0a98e57004e7d5e409",
      "67c18d106aaf4ae7baa9b230ae3fdd02",
      "027c176eb68c4f2dac2b2f3217e318ce",
      "830b475f509843afbdd708da9ea0cfe7",
      "1a1d8025f0a744988f99e393f5eb0e9d",
      "69379bacf88a442cb9c84a5a96879570",
      "2ed88daadce14e20ba6c448f7d938e76",
      "d9c7bc2a441b420e9a960d001f465b16",
      "bb564f5ffe4e40baac80b021a03c6c88",
      "bcf151eab4214bdf84dfcdfb4af91cb9",
      "bdf16783cbce4e548068fd46349baef5",
      "de343a9d40ea48baa0dd128601575eaa",
      "90a573216fd44732b9eb9cacf9268236",
      "b4b7a8209c2045b3acbd56f921d3d7c9",
      "186964fcc07e4d9fbbcad9095cff6ae6",
      "80668726066d4720b512a185de93ba41"
     ]
    },
    "id": "5b382dcd",
    "outputId": "510305c3-716c-43e4-9e86-4e95e86f815d"
   },
   "outputs": [],
   "source": [
    "for dataset_name, dataset in datasets.items():\n",
    "  print(dataset_name)\n",
    "\n",
    "  for model in models:\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    result = run_hpo_pipeline(dataset_name, dataset, model)\n",
    "    result.save_to_directory(f'{dataset_name}/{model[0]}')\n",
    "    if (SAVE_TO_DRIVE):\n",
    "        result.save_to_directory(f'/content/drive/MyDrive/hpo_results/{dataset_name}/{model[0]}')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Final Project - Embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
